[{"title":"Reading:A 5-Point Minimal Solver for Event Camera Relative Motion Estimation","url":"/2024/06/02/A-5-Point-Minimal-Solver-for-Event-Camera-Relative-Motion-Estimation/","content":"Original Paper Link:Paper Paper Auther: Ling Gao* ,Hang Su* ,Daniel Gehrig, Marco Cannici, Davide Scaramuzza, Laurent Kneip\nPreliminaries\nPlucker CoordinatesA Line L can be represented by its direction vector d and a point PFor two nonparallel lines  and , we have\nEvent-based Data NotationsConsider data in , the observed events are given by , where -th event of the -th cluster Function here projects points  defined in the camera frame into the image frameConveresely haveAnd  represents the translational velocity and  represents the rotational velocity.\nIncidence RelationshipCamera center at time Rotation from camera frame to reference frameAnd we also haveTherefore, the ray can be described in plucker coordinates$$\\left( \\left[ \\mathbf{R}[t_{ij}] \\mathbf{f}{ij} \\right]^{\\top} \\left( \\mathbf{C}[t{ij}] \\times \\left( \\mathbf{R}[t_{ij}] \\mathbf{f}{ij} \\right) \\right) \\right)^{\\top}\\left\\langle \\mathbf{d}i , \\mathbf{C}[t{ij}] \\times \\left( \\mathbf{R}[t{ij}] \\mathbf{f}{ij} \\right) \\right\\rangle + \\left\\langle \\mathbf{R}[t{ij}] \\mathbf{f}{ij} , \\mathbf{m}i \\right\\rangle = 0\\left\\langle \\mathbf{d}i , \\left( \\mathbf{v} \\cdot t’{ij} \\right) \\times \\mathbf{f}’{ij} \\right\\rangle + \\left\\langle \\mathbf{f}’{ij} , \\mathbf{m}_i \\right\\rangle = 0,$$\nTransition to Minimal FormThere are two points in the reference frameAnd we define the base of three new directionsSince the velocity in x directions cannot be observed, we have$$\\mathbf{v} = \\left[ \\mathbf{e}_1^{\\ell} \\quad \\mathbf{e}2^{\\ell} \\quad \\mathbf{e}3^{\\ell} \\right] \\cdot \\left[ 0 \\quad v_y^{\\ell} \\quad v_z^{\\ell} \\right]^{\\top} = \\mathbf{R}{\\ell} \\mathbf{v}{\\ell}.t’_j \\left( \\mathbf{P}b - \\mathbf{P}a \\right)^{\\top} \\left( \\left( \\mathbf{R}{\\ell} \\mathbf{v}{\\ell} \\right) \\times \\mathbf{f}’j \\right) - \\mathbf{f}’{j} {}^{T}(P_b \\times P_a)=0$$\nFive Point Minimal SolverTo remove the scale invariance, an additional constraint on the scale is added. Given that only the structure parameters are affected by the scale invariance, the scale constraint needs to include the related variables. We constrain the scale by adding the equation$$\\left( \\mathbf{R}{\\ell} \\mathbf{v}{\\ell} \\right)^{\\top} \\cdot \\mathbf{R}{\\ell} \\mathbf{v}{\\ell} - 1 = 0.\\mathbf{v} = \\mathbf{e}{1i}^{\\ell} \\cdot \\kappa_i + \\mathbf{e}{2i}^{\\ell} \\cdot v_{yi}^{\\ell} + \\mathbf{e}{3i}^{\\ell} \\cdot v{zi}^{\\ell}.\\begin{cases}\\mathbf{e}{2i}^{\\ell \\top} \\mathbf{v} = \\mathbf{e}{2i}^{\\ell \\top} \\mathbf{e}{1i}^{\\ell} \\cdot \\kappa_i + \\mathbf{e}{2i}^{\\ell \\top} \\mathbf{e}{2i}^{\\ell} \\cdot v{yi}^{\\ell} + \\mathbf{e}{2i}^{\\ell \\top} \\mathbf{e}{3i}^{\\ell} \\cdot v_{zi}^{\\ell} \\\\mathbf{e}{3i}^{\\ell \\top} \\mathbf{v} = \\mathbf{e}{3i}^{\\ell \\top} \\mathbf{e}{1i}^{\\ell} \\cdot \\kappa_i + \\mathbf{e}{3i}^{\\ell \\top} \\mathbf{e}{2i}^{\\ell} \\cdot v{yi}^{\\ell} + \\mathbf{e}{3i}^{\\ell \\top} \\mathbf{e}{3i}^{\\ell} \\cdot v_{zi}^{\\ell}\\end{cases}\\Leftrightarrow\\begin{cases}\\mathbf{e}{2i}^{\\ell \\top} \\mathbf{v} = \\left| \\mathbf{e}{2i}^{\\ell} \\right|2^2 \\cdot v{yi}^{\\ell} \\\\mathbf{e}{3i}^{\\ell \\top} \\mathbf{v} = \\left| \\mathbf{e}{3i}^{\\ell} \\right|2^2 \\cdot v{zi}^{\\ell}\\end{cases}\\Leftrightarrow\\begin{cases}\\left| \\mathbf{e}{2i}^{\\ell} \\right|2^{-2} \\cdot \\mathbf{e}{2i}^{\\ell \\top} \\mathbf{v} = v{yi}^{\\ell} \\\\left| \\mathbf{e}{3i}^{\\ell} \\right|2^{-2} \\cdot \\mathbf{e}{3i}^{\\ell \\top} \\mathbf{v} = v{zi}^{\\ell}\\end{cases}.\\begin{bmatrix}\\left| \\mathbf{e}{21}^{\\ell} \\right|2^{-2} \\cdot \\mathbf{e}{21}^{\\ell \\top} &amp; -v{y1}^{\\ell} &amp; \\cdots &amp; 0 \\\\left| \\mathbf{e}{31}^{\\ell} \\right|2^{-2} \\cdot \\mathbf{e}{31}^{\\ell \\top} &amp; -v{z1}^{\\ell} &amp; \\cdots &amp; 0 \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\left| \\mathbf{e}{2N}^{\\ell} \\right|2^{-2} \\cdot \\mathbf{e}{2N}^{\\ell \\top} &amp; 0 &amp; \\cdots &amp; -v{yN}^{\\ell} \\\\left| \\mathbf{e}{3N}^{\\ell} \\right|2^{-2} \\cdot \\mathbf{e}{3N}^{\\ell \\top} &amp; 0 &amp; \\cdots &amp; -v{zN}^{\\ell}\\end{bmatrix} = 0.[A\\quad B][…]=0 = 0, \\quad \\text{where}\\left[ \\mathbf{U} - \\mathbf{W} \\mathbf{V}^{-1} \\mathbf{W}^{\\top} \\right] \\mathbf{v} = 0,$$\n","categories":["Lab reading"]},{"title":"Abstract Algebra: Midterm I Review","url":"/2024/09/23/Abstract-Algebra-Midterm-I-Review/","content":"DefinitionsSet \n              \n              A set is a collection of elements   \n              \n            \nMap \n              \n              A map  from  to  is a rule which assigns a unique element of  to an elements of .  surjective: iff   injective: iff if , then  if , then 3.bijective: iff  and \n              \n              \n\nProduct \n              \n              A product of  and  is a set   \n              \n            \n\nEquivalence Relations \n              \n              An equivalence relation  (or ) on  is a subset 1.reflexivity: 2.symmetry: if , then 3. transitivity: if , then   \n              \n            \n\nEquivalence Class \n              \n              An equivalence class of  is   \n              \n            \n\nQuotient \n              \n              A quotient of  by  is   \n              \n            \n\nZ/nZ \n              \n               iff   \n              \n            \n\nGroup \n              \n              A group  is a set G and a binary operation  s.t. ,  Associativity:  Identity:  Inverses:   \n              \n            \n\nHomomorphism \n              \n              A homomorphism from  to  is a map (of sets) ,s.t.\n              \n            \n\nSymmetric Groups \n              \n              Let  be a set, we define (cardinality of X=number of elements in X) is a group of all bijective maps (bijections)   \n              \n            \n\n\nSubgroup \n              \n               is a group, Subgroup  of  is a group  with the group operation restricted from .\n              \n            \n\n\nSubgroup Diagram \n              \n              Subgroup diagram of a group ,trival subgroup() , is contained as a set. They are all subgroups of   \n              \n            \n\nProper Subgroup \n              \n              Proper subgroup of  is subgroup which is not trivial or  itself.  \n              \n            \n\nCyclic Subgroup \n              \n              $Ga{a^k | k \\in Z}\\star = \\star_G$ \n              \n            \n\nOrder \n              \n              ,Order of $$ is minimal ,s.t. \n              \n            \n\nIsomorphism \n              \n              Isomorphism is bijective homomorphism.(which is bijective：injective,subjective)  \n              \n            \n\nKernel \n              \n              Kernel of , Kernel is the preimage of identity: \n              \n            \n\nCoset \n              \n              ,  is the coset of .  \n              \n            \nDirect Product \n              \n               are groups, direct product of  &amp;  is a set  with the following operation   \n              \n            \n\nNormal \n              \n              ,  id normal iff iff  \n              \n             \nKernel And Image \n              \n              if  is a homomorphism image of  kernel of  \n              \n             \n\n\nInner Direct Product \n              \n               is a group, ,  is the internal direct product of  and  iff1.(warning: )2.(i.e. as samll as possible)3. for any (warning: G is not abelian)   \n              \n             \n\nPropositionsProposition1Proof has exactly n elements   if , then Let , thenthen, if  and , then [i] and [j] do not intersect We prove by contradiction:assume then,According to , we have Since a and b are integers, we haveand since we have,which caused a contradiction, and it should be . Proved. Every  belongs to one of the classes Proof:By ,  has at least n elements.By ,  has at most n elements. \n\nProposition2ProofAddtion is correctly define on  ,the class  is the same for diff. elements of  and .\n\n\nProposition3ProofIdentity element e is laways unique in any group G. By contradictionwhich caused a contradiction \n\nProposition3.18Proof is a group. The inverse  is unique By contradictionwhich caused a  contradiction that \n\n\nProposition3.19ProofFor any   \n\nProposition3.20Proof\n\nProposition3.21ProofFor any   \n\nProposition3.22Proof \n\nProposition\nOperation of composition (of maps) is associative   \nIdentity:   \nInverses: Just flip it.\n\n\n\nProposition3.30Proof is a subgroup of  iff:1.(identity of subgroup)2.(closure of the group operation)if , then ,3.(closure of inverses)if , then .  is Problem #2  on HW#3  is a subgroup  that all of three is true  \n\nProposition3.31Proof is a subgroup iff () and ()\n\nLemmaProof$={a^k|Z}$ is a subgroup of G  if , then \n\nLemmaProof$={a^k|Z}$ is a subgroup of G  if , then \n\nProposition is cyclic iff $\\exists g \\in G, G= $ \n\nPropositionProofare groups and they are homomorphism, then:1.2.3.if , then   (image of  under )3’. as , then (image of  : )4. if , then  is a subgroup of G (we call  is called pre-image of ) 1.2.3.4. Let us consider  is a subgroup?Prop 3.31: \n\n  \n    \n      Lemma6.3\n\n    \n    \n      The following are equivalent(1-3 therioticallt useful, 4 griginal warm-up, 5 How to use this lemma for actual )  \n\n    \n    \n\n\nCorollaryProof if  is abelian, then every subgroup is normal.   We have , as Q: Why do we need the definition of normalityIdea: , , we want group operation on (factor/quotient group of ):such that Problem: if How to deal with such sits? Whetheror not.\n(Remark: , it is really good!)\n\n\nTheoremTheoremProof$H \\leq G \\quad s.t. \\quad  a \\in H$ part is already proved.Let us prove the second part.Any subgroup , then $$\\forall k \\in Z, a^k \\in H, \\text{then}  \\leq H$$Therefore $HGa \\in H$\n\n\nTheoremEvery cyclic group is abelian, \n\n\n  \n    \n      Theorem 6.4\n\n    \n    \n      H-cosets partition G: we can find  a set of elements of  such that  that \n\n    \n   \n\n\n\n  \n    \n      Theorem 6.10\n\n    \n    \n      Lagrange’s Theorem= finite group, , (Note: all three are natural numbers)  \n\n    \n    \n\n  \n    \n      Cor 6.11\n\n    \n    \n      , (Remember, = order of =minimal  s.t. )  \n\n    \n   \n\n\n\n  \n    \n      Theorem 9.7+9.8\n\n    \n    \n      If  is a cyclic group, then: if it is finite order(n), then it is isomorphic to . if it is of infinite order, then it is isomorphic to .\n\n    \n    \n\n  \n    \n      Thm/Prop\n\n    \n    \n       if  are isomorphic, then,then other is. if  are finite, then,then other is. if  are cyclic, then,then other is.\n\n    \n    \n\n\nTheoremProof if  is a homomorphism, then  ( means isomorphic,  is bijective)\n is normal, 2.To find \n\n\nTheoremProofif  is internal direct product of , then  Let’s consider Q: is it well-defined?A:   , by 2, we know thatQ:homomorphism?+surj+inj\n\n\nPropositionProof内容内容\n\nZ/nZ \n              \n               iff   \n              \n            ","categories":["Math113-Abstract Algebra"]},{"title":"CS170 Cheatsheet","url":"/2024/12/12/CS170-FInal-Pack/","content":"\n\n  37f12a4bf8210246f9d1e6577d2e1c7e31a8383c8beb6732fff3551792eb4dd96015e0fb8cb64b967cf309fa61bf767c5ed817f70496ddd9ac9b064bf2c7e4ce1b3414bf9ba942796a867e66a5f50423f8e82932beee0961ba81fbac4a604080532ca337cf9abce04e452d6824b6cdef3bb22c582ed33cbc587a5480e56b136b8c01ced9fd38ff8a2e244d831604df8865f1fa204c8a774fbb2f98a65c0591b06e13b35c7ef281bd04c0138a60145629f7feea310e87cbc4f0ef507143ee8ebfc111a6fdb211f68ed7e7ba4e14873a18151c43ed1a21cf70cf908f5f40d7a1df92e8c39058562f5906e13882e18e61e608d992fc95f8199b6bafa47c23e088e4f7c67639bd0392813176007a6c0df1e3c5967817e12d1956366c0ac168ed686858b287d4e526ddb277aad785bd7979ad705c6a0df1f2325343469f14f7d68614966f14e727e241d00d90cfc91db78bff8c0a555f041ba403b8dddc37aae9b1c86ee0ca90529fee083de2bc4c39522721a2ca8461fd8a02d7e128259661986b2036eef7f862658deae2b3d9f926602487fbabff18d84fcaf4532c65a64b0c9deee5ec8a4f45a5a239ec1ae82701d3731b806efa758871c44aef155280825980aca43f6a42a4f54d985e9d9e48e216c5173da122d1dafe1371532337ced31171cb2ec101a8adac8b9937c0d40e48b11af53497217ad30fc2811cfe8775c76858940a5be9a4871cf504242faa466925bb9b3196eee52fbf3b455f4fc7e68570a95b9f38fb9cf22b4746dea304f803c6a8c6e4cef93071d4b4067dee22815ec346b4dd624d66346ffed0960ca90404baa150f2521717bc45e8c1e0e7b165b6ec1897becc5748405a48caf4beeca126afd69ea257de56ac08c0f8ed9596aaa210fec0aebec6026f762e1629cc1f78001edb9919a40f010b67203c8ae987befe44fe83b4a1be3694ecb7324ff0be0a30f95481dc1121b67933b19b8cb23be42963f6d9\n  \n    \n      \n      \n        Hey, password is required here.\n      \n    \n  \n\n","categories":["Efficient Algorithms and Intractable Problems"]},{"title":"Deep Neural Networks(CS182-Review)","url":"/2025/04/18/CS182-Final-Review/","content":"\nBasicsContentsHomeworksDiscussionsInitialization:ContentsHomeworksDiscussionsSGDContentsHomeworksDiscussionsOptimizerContentsHomeworksDiscussionsCNNContentsHomeworksDiscussionsGNNContentsHomeworksDiscussionsRNNContentsRNN IntroductionRNN are designed for sequential data. In conv-net, we had filters defined by finite convolutions, i.e. FIR filters. In RNN, we had filters defined by internal state and weights, i.e. IIR filters.  \nKalman Filter ExampleYou have linear evolution of a true system with a description aswhere  are the underlying (hidden) states of the system,  are some driving terms (e.g. white noise), and  and  are some coefficient matrices.A noisy observation is\nRNNwriting the RNN as a block matrix multiplication\nand replace it by MLPs. Now, a box in the network becomes\nRecall the structure of an RNN layer that takes in the input of the given time step and the hidden state:\n\nThe back  propagation in RNN follows the same rule as any other neural networks,\nRemark: Downsampling is not applicable in RNNs, because sequential inputs, unlike images or graphs, are not strongly connected at all regardless of the scenario.\nIncreasing Expressiveness in RNNWe can modify the transformation , by either appending an MLP before returning each hidden state, or if  were an MLP by adding the number of the dimensions in the output.A more widely implemented method today is to compose the hidden states into more RNNs, thus providing both a better way of processing data as well as giving out a more intuitive explanation for weights. Namely, we can add horizontal layer.  \nDealing with Exploding GradientsThe most common way to deal with exploding gradients is to use saturating activation functions, which have a small gradient when the input is large. In other words, we are replacing ReLU with a function such as tanh or sigmoid.Gradient clipping is a standard technique to deal with exploding gradients in any kind of neural network.This requires simply clipping the gradients if they get above a certain value.In the case of RNNs, this will stop the gradient from exploding, but it may have unintended consequences because it will distort the “shape” on which gradient descent moves. Therefore, historically, people use tanh/sigmoid for RNNs, and gradient clipping can be seen as a last resort.Similar to CNN, we can also place a normalization layer between layers of an RNN, which helps deal with exploding gradients.However, there is an issue because without the entire sequence available, normalization is impossible, since we don’t know all the values over which we want to normalize. One way to deal with this is to substitute the predicted effects of all other inputs we haven;t seen yet at test time to perform the normalization.Another way to deal with this is mini layer norm,Mini vertical norm is the same as mini layer norm, except instead of normalizing a hidden state’s path to the next layer, it normalizes its path to the next sequence step.And please note that this vertical norm is not done in practice. Because there is no issue with computation, it is just not practical or desired to mix the effect of different learned filters.And for batch norm, this is possibe, but not usually done. Because it is more difficult to implement than other forms of normalization because sequences from different samples could have different lengths.\nDealing with Dying GradientsWe can apply the same thing like residual blocks into RNNs as well. The picture below shows vertical skipsThe picture below shows horizontal skips. It is true mathematically and helps with dying gradients, but it may change the inductive bias of the RNN.With RNNs, we typically want the learned weights to act primarily locally (in other words, learn mostly based on the input they directly receive at that sequence step). However, horizontal skips weaken this locality by bringing in an input from earlier in the sequence that will be less “dead” than the gradient coming from the sequence step directly previous. Put concisely, horizontal skips mean less favoring of local information (inputs nearby in sequence position).Is this a bad thing? It depends on the application. If it relies heavily on local data (short-term dependence), we would need a lot more data to force the RNN to look locally instead of using gradients from the past. If it requires more long-term dependence, then we are okay.\nDealing with multi-scale Information(LSTM) OptionalInformation can be gleaned at different levels of precision(e.g. sentence level, phrase level.) This presents some challenges:1. Context in the data is long-lived (e.g. a full sentence), but can shift suddenly (e.g. that sentence ends and a new one starts). 2. Details that the RNN is looking for evolve and mix at different points in the sequence.\nHomeworksBackprop through a Simple RNNConsidering the following 1D RNN with no nonlinearities. The formula is given by:Ans:\nDiscussionSeq-to-Seq ProblemsContentsThe goal is to map an input sequence of data to an output sequence.(e.g. predicting, generating a sequence). It’s natural to consider RNN for seq-to-seq problems because RNNs are known to be useful using RNNs for seq-to-seq problems.\nMachine Translation Example(Challenges)There are potential challenges in RNN when utilizing it into machine translation tasks: 1. sequential processing process sequences one element at a time, which can result in slow computation for long sentences. 2. RNNs have a limited context window, which makes it difficult to capture relationships between words and phrases in longer sentences. 3. the order of the words in the source and target might be different, making it challenging for RNNs to learn correct alignment.\nEncoder-Decoder ArchitectureEach word is inputted into the encoder blocls sequentially and each block outputs a hidden state. Each block shares the same weight.Decoder takes the input representation and generates the output sequence one element at a time.  This allows the RNN decoder to capture dependencies and relationships between elements in the output sequence. \nTeacher ForcingWhile training decoders, in order to avoid accumulative errors, we use the ground-truth tokens as input for the next time step. \nAuto-RegressionAutoregression is a technique that instead of using ground-truth tokens as input to the decoder at each time stamp, its tokens are generated by the decoder itself. Using noisy inputs helps the robustness of the model, which have a result ithat when received an imperfect token, the after parts can still stay stable. What’s more, instead of over relying on clean data inputs, it helps better predictions when dealing with small errors. \nCross-Entropy Loss for Language ProblemsFor sequence-to-sequence language generation tasks, the output data is discrete (e.g., words or characters), so cross-entropy loss is a very suitable choice. At each time step, the model produces a probability distribution over all possible tokens. We then use the one-hot representation of the ground-truth token to compute the loss:\n\nDuring inference, if we use autoregression, we can no longer feed the full probability distribution as input to the next time step — instead, we must convert the distribution into a specific token. Common methods include:\n\nGreedy decoding: Select the token with the highest probability    \nSampling: Sample a token from the probability distribution    \nBeam search: Keep a small number of top candidate sequences and explore them in parallel\n\nHomeworksDiscussionsAttention MechanismContentsIn language processing ,different languages have varying word orders, making the routing of information context-dependent. To address this issue, the goal is to add a “memory” to the existing Encoder-Decoder architecture that allows the input information to be stored and retrieved at the decoder. This allows the decoder has access to the right piece of information for every position, and facilitating the generation of high-quality output.\nQueries-Keys-ValuesUnlike the traditional hash table, our query may not always match to keys in the hash table, so we need to design a differentiable hash table which not only can get an appropriate forward value, but also backpropagate to make it work while training.\nGreat Ideas in AttentionIdead(-1):look for an exact match for query and key and return a valueThis doesn’t works since we always fail to get the return value because we don’t have the exact matching keys and queries. Additionally, we are unable to calculate the gradient using back-propagations since we won’t get any matches.Idea(0): scan for the closest match for query to key in the hash table and return the valuewe can search for a value now. But we are using the similarity between query and key, which means small changes to the query or the key will result in the same output, and the gradient will be 0 as well.Idea(1):Scan for the closest matches of query to the keys. Then return a weighted average of the values.In this way, attention can be thought of as “queryable pooling”, because keys and query values are also learnable parameters, but there are no learnable weights in the attention mechanism itself.Picture below shows the attention mechanism in RNNs.In this way, we haveHere, the similarity function can be softmax or some kernel-based methods. In attention, we will use the normalized inner product,where d is the dimensions of q and k.(Here according to the central limit theorem, the product has a standard deviation that is proportional to the square root of the number of variables being added together. So normalizing by dividing the root of the dimension keeps the score within a reasonable range).After that, we use softmax to compute the weights:To get the answer, we can simply say\nCross-Attention and Self-AttentionIf keys and values are obtained from the encoder, we call the attention layer Cross Attention. The keyand the value can use the same information as the input (like layer output / input etc.), but be generatedthrough different attention layers, so that they can focus on different parts of the info. If the key and thevalue are from the decoder, we call it Self Attention. The benefit of this includes creating more routes toprevent the gradient from dying. Note that in this way the key and the value must be causal, which meansthey can only depend on the previous or current timestep. If time is current, only layers below can be seenas the input. In a word, only the input side of the decoder can be used here.In Cross Attention, there are two ways for the gradients to go backwards: One through the hidden statesthrough the RNN encoder, and the other through the attention mechanism and the key / value to theencoder. The hidden-state route can keep track of the time info (e.g. what’s the last word input?), while theattention is normally unordered across input subscript i. However, we can also add time to the attentioninput directly, which is called positional encoding.\nHomeworksKernelized Linear Attention(Part II)Ans:  \n\n\n\n\n\n\n\n\n\n\n\nNote that  and  can be computed by  in constant time hence making the computational complexity of linear transformers with causal masking linear with respect to the sequence length.\nDiscussionsSelf-supervisionContentsHomeworksDiscussionsTransformerContentsPositional EncodingPositional Encoding solves the problem that how to add time to the input vector. Just increment a counter. Like mark the first place 1, the second place 2, etc. However, the number will becomes super large like 1000 fir the 1000th place. And it will cause an unbalanced encoding as the later places would be shining in the model. Normalized counter. We set the largest step as 1 and the smallest number as 0. In this way, like for 1000, the difference between each step would be very tiny: just 0.001. Use circular time. We haveIn this way, the norm of the positional encoding is always 1. However, the difference between two steps would still be very tiny when w goes. However, if  is super small, the difference between two steps are still tiny, which means positional codes between two steps almost overlap and lost the ability for small-scale clarification. Use an ordered list to store the key and the value instead of using unordered dictionar.\nHomeworksDiscussionsFine-tuningContentsFine-tuningTransformer and its training are very hungry for training data, as they have fewer assumption to data structures like CNN and RNN, so they have a much weaker inductive bias. One way to solve this is take advantage of unlabeled data like self-supervision. Another way is amortization across tasks. In practice, the transformer is pre-trained on some tasks and possibly by self-supervision, then do the adjustment(fine-tuning) for the task we specifically care about. The basic method is adding a new head as follows. We remove the old head, add a new head with random initialization and then train new head on rask-specific data, while freezing the rest of the net.One way to think about this is that the last linear layer is the embedding of the learned features. We can choose anywhere along the newral network as the end of the embedding and remove the following structure as the old head.\nLinear-ProbingClassic way is to treat the pre-trained model as a feature extractor. We just train the head, freezing all the weight outside of those in the task-specific head.Ways of generating features:1. generate features from the output of the penultimate layer; 2. use outputs from previous layers; 3. average the outputs of various layers together.\nFull Fine-TuningTreat the pre-trained model as a good starting point as initialization. However, even if we train everything together, we still need to initialize the head. Here are several ways:Approach #1: random initialization of the task specific head and begin the fine-tuning process. But it may send bad gradients through to the pre=trained model, potentially causing harmful changes to the pre-trained weights.Approach #2: first random initialization and only train head, then train all the remaining and do full fine-tuning.Classic method is faster and have lower requirement to the memory needed, while full fine-tuning have a higher accuracy but requires larger memory.\nUpdating A Subset of WeightsWe can improve the pre-trained model itself by increasing the size of the network, using more data, using better and cleaner data or more proxy tasks, meta-learning, etc.More than that, we can do something between full fine-tuning and classic approaches. (like just update a subset of weights in the pre-trained model). There are possible ways to determine which subset to update:  \n\nUpdate a different set of weights as you go.(Similar to SGD, only update part of the informations.)  \nSelectively unfreezing the model top-down, like fine-tune top k layers. However, recent paper shows that unfreezing the bottom layers may also work when the training data and the actual data have different distributions.  \nSelectively unfreeze Attention. When we use transformer, if we have different context-dependence compared to original training task, we can choose to unfreeze the attention components.\n\nLoRAWhen we are doing fine-tuning, we are most likely be moving in a subset of directions, which will approximate a low-rank update anyways by moving the important parameters a lot and moving the less important parameters very little. In order to perform LoRA, we introduce factorizations to our parameters,\n\n\n  \nwhere  is a  matrix,  is frozen, and  is  parameters.LoRA can be generalized to what people call adaptors. These are other small structures put inside the pre-trained model to be tuned, and keeping everything else frozen. One way to think about it is:  \n\n\n  \nSVD is a sum of rank 1 updates, starting with the most important updates.\nQuantized ModelIt’s a method used for compression. In compression, there are 2 things happening:1. transform the domain;2. use quantization to give appropriate precision to the places where there is action and nothing to places where there is no action. These models can be thought of as a glorified PCA. When using weight decay, the model will learn to favor specific directions. When performing full fine-tuning, we are locally working with a linearized version of the model around the neighborhood of the initialized weights. If a model has billions of parameters, it is most likely not having an equal amount of motion along all the different weight directions. There are likely somedirections that are more important than others.\nHomeworksDiscussionsEmbeddingContentsGPT-Style EmbeddingGPT-style is the self-supervise or auto-complete by next word/token prediction like the system-id style.For language problems, we wish to get vectors from the text, which is called the problem of tokenizaiton and token embedding. This is done by parsing the input string into segments of tokens and followed by a look-up table that could be tuned/learned.GPT-style embedding starts with &lt;START&gt;  and outputs with a loss(cross entropy) between the first prediction and , then the network has  as input and predict the next. One benefit is that all the text could be used as the training data to do the next token prediction. The underlying lower dimensional structure or the embedding of language is presumably learned after training. The attributes of language are distilled from the autocomplete (next token prediction). One comment is that way more features than the training data may be obtained after this but that should be okay in our context. We may assumethe regularization works.\nBERT-Style EmbeddingBERT Style does the data augmentation, such as the noisy and masked auto-endoding we have learned. Some fractions of the input are masked and the task is to reconstruct the embedding.(like  here), the input like  could be replaced with the mask which is a real vector.\nTokenizationTokenization is necessary because textual information is given as sequence of strings from a discrete alphabet. There are 2 steps:1. parse the strings to a sequence of tokens. 2. map those tokens into vectors.A natural way is to use letters of the alphabet as tokens, but the single letter is not a meaningful token. If we could have each token represent a meaningful unit instead, we effectively do this work for the model.We do that via a lookup table, where each of the m possible inputs is mapped to an index. And we want to ensure prefix freeness(i.e. no string is the prefix of another, so that there will be no ambiguity between “a” and “an”.)The we have the token as an index, we map those tokens to vectors.Mapping vectors is easy, but the look-up table here in not a linear map, like the left column is discrete strings and cannot be calculated by gradient passing. One way to contruct such a linear map Byte Pair Encoding, which is similar to huffman encoding. The intuition is that occur commonly together do so because they represent a semantically meaningful unit.Note that tokenization is done before training our model. Usually we just use the current tokenizers like OpenAI’s tiktoken.\nWord2VecWe achieve this in the following way:  \n\nRandomly initialize two vectors  and  for each word.\n\nUse  to measure a score for  as a likely neighbor for .\n\nTrain using “logistic loss style” loss, i.e.with randomly selected word . We compare  with positive examples that are its neighbors and random negative examples. We need to train with both positive and negative examples in order for the vectors to not all be , which would minimize the inner product between all vectors.   \n\nUse the average of  and  as the final embedding.\n\n\nBERTBERT is an encoder-style transformer, which means self-attention is not causal. Text input is first transformed into a vector(with learnable parameters) and concatenated to a positional embedding(without learnable parameters). The core model is composed of a stack of attention blocks with serial (or parallel) interconnections to NLP. At the end is a task-specific MLP that maps the embedding to scores for each token.BERT is originally pre-trained on 2 tasks: one is masked denoising(predict the masked words), and one is changing the sentence order(predict if the order of two sentence is changed or not).Two ways we utilizing BERT might be feature extraction and fine-tuning. Feature extractors. Treat the embeddings of the language model as a feature extractor and use any combination of the activations as an input to a separate model. While you could just use the penultimate layer, you could also use the last N layers and concatenate or average them. Selecting a subset of features becomes its own hyperparameter search.Fine-tuning. Use BERT as a building block. Replace the task head (in this context, the head refers to the MLP fine-tuned to a specific task) with a new MLP. Fine-tuning can apply to just the final layer or to the entire model, but fine-tuning the entire model can actually degrade performance. This is because if the final layer’s weights are poor, the backpropagated weight updates may also be noisy and suboptimal. The present best practice is to freeze the pre-trained model first, and then traineverything.\nHomeworksDiscussionsPromptingContentsGPT-Style ModelsGPT-style moels are trained on the task of auto-complete(next token prediction).Zero-shot learning is an attempt to learn a task from no previous examples.(eg: prompt: the capital of france is. model returns: paris). In this example. the sentence generated from the following templateThe entire pipeline with a blackbox GPT model can be seen below:The area of generating prompts to solve specific tasks is generally known as prompt engineering.  \nFew-shot LearningFew-shot Learning is an idea that provide the model some, but not much training data for the task through the prompt itself. The hope is that additional context will direct the model to provide better answers. Ex:While the performance of few-shot learning is better than that of zero-shot learning, theperformance is still not great. This may be driven by two issues.First issue is that the tranining data is greater than the context length of the model, which results in the training data won’t fit in the prompt. We can split the data into k batches, then feed one batch at a time and treat output as a combination.(We treat the GPT as black-box, without any updating in the inner weights).The second issue is that we provide prompts in human-speech through tokens, but the natural language of computers is vectors. To address this issue, we choose to allow the prompts themselves to learn via GD, which is called .With soft-prompting:1. performance dramatically increases;2. only requires us to store a small number of parameters;3. memory usage is higher.\nHomeworksDiscussionsCatastrophic ForgettingContentsWhat is CF?When a model learns something new, it can forget something it already knows. This phenomenon can be viewed as both a feature and a bug, and here we treated it as catastrophic forgetting as a bug.(like we trained on recognizing handwritten 1, but when we then train on 2s, then both of the loss of 1 and 2 will increase).Approaches for training on multiple tasks:1.freezing the model;2.linear probing strategy;3.soft prompt before pre-trained model;4.low-rank adapter.  \nHomeworksDiscussionsKnowledge DistillationContentsHomeworksDiscussionsMeta LearningContentsIdea of Meta-LearningIn meta-learning, fine-tuning is central (“on a task you can see”) as meta-learning means to learn how to learn. In practice, this means “we want to be trained so that we are good at being fine-tunable”.Note: this is a distinction from “post-train” fine-tuning where the aim is to modify parameters after the main training is done. Now, the fine-tuning happens as we train since we know we are going to fine-tune anyways. This saves time and will in most cases produce better models.The distinction here is that latter perspective treats fine-tuning simply as an interesting emergent property, while the meta-learning perspective considers optimizing fine-tuning as we train the model. So, this leads to the question of how to do this in practice.When fine‑tuning with very little data, the model’s updates concentrate along a handful of dominant directions—the top singular vectors of its locally linearized input–Jacobian. If two tasks share these principal directions, gradient steps for the new task will overwrite the weights needed for the old task, causing catastrophic forgetting. Conversely, if each task’s dominant directions are orthogonal, fine‑tuning on one task induces negligible perturbation along the other task’s directions and thus preserves prior knowledge.\nMAML\n‑   \nAssume we have  tasks (Task , Task , …, Task ), each with its own training and validation data , where  indexes the task and  the sample.\n\nFor each task , starting from the shared initialization , perform  steps of gradient descent on the task’s training loss to obtain task‑specific parameters :  \n\n   \n   \n\n‑Evaluate each  on its validation set, sum the validation losses, and update the initialization  via gradient descent:  \n\n   \n   \n\nMAML uses only standard gradient updates, making it compatible with any differentiable model—be it CNNs, RNNs, Transformers, or reinforcement‑learning policies.\n\n\nSemi-FrozenIn the semi‑frozen approach, we begin by unfreezing the shared, pre‑trained backbone during multi‑task training so that it can accumulate updates across all tasks and converge to an optimal initialization. Once this “best” checkpoint is reached, we freeze the backbone again at test time: for each new task, we restore the checkpoint, fine‑tune only the task‑specific components (or head), and leave the shared layers untouched. Whenever another task arrives, we simply reload the same checkpoint and repeat the local fine‑tuning. By combining an initial phase of full adaptability with a later phase of targeted, frozen‑backbone tuning, this method strikes a balance between the stability of self‑supervised pre‑training and the flexibility of task‑specific adaptation.\nSubset StrategyIn the subset strategy, when a task’s full training set is too large to traverse in only K inner‐loop steps, we randomly sample a smaller subset of examples (e.g. 100 out of 1,000) and perform our SGD‐based fine‑tuning on just that mini‑batch. This lets us respect memory and compute limits while still giving the model a representative glimpse of the task. However, because we never see the task in its entirety during those K steps, we can’t fully measure how the initial initialization would behave on all data, and our “exploratory” updates become only a rough approximation of full fine‑tuning—making the algorithm practically resemble standard SGD more than true meta‑learning.  \nReptile StrategyIn the Reptile strategy, instead of computing the full meta‑gradient , we simply take the difference between the fine‑tuned weights and the initialization () as a proxy for the update direction. After running K inner‑loop SGD steps on a sampled task to reach , we move the shared parameters slightly toward  using this vector. By repeating this across many tasks—and optionally unrolling  steps—we approximate the effect of second‑order gradients without ever forming or inverting a Hessian, thereby avoiding exploding gradients and greatly simplifying implementation.\nClosed Form StrategyIn the closed‑form strategy, we treat the frozen shared backbone as a fixed feature extractor and train only lightweight, task‑specific heads (e.g. linear regression) by directly applying the closed‑form solution of a convex problem, such as least squares: . Because this analytic formula is differentiable, we can still backpropagate to compute gradients with respect to the initialization , all without performing iterative SGD steps. This both removes exploding‑gradient risk and drastically reduces computation and memory overhead, yielding a concise and reliable fine‑tuning mechanism tailored to each task.\nHomeworksMeta-Learning For Learning 1D FunctionsAns:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussionsTransfer LearningContentsHomeworksDiscussionsGenerative ModelsContentsHomeworksDiscussionsDiffusion ModelsContentsHomeworksDiscussions"},{"title":"Introduction to Abstract Algebra","url":"/2024/09/07/Abstract-Algebra/","content":"Lecture 1Warm-upLecture 3Warm-up\n\n False, \n False, \n? \nDescribe  as a subset of .  , the last digit is  in base \n\nNotation iff \nZ/nZ: Act IIRemark:  consists of subsets of    \n\n  \n    \n      Proposition\n\n    \n    \n       has exactly n elements   \n\n    \n    \nProof\n  \n    \n      claim 1\n\n    \n    \n      if , then     \n\n    \n    \nLet , thenthen,   \n\n  \n    \n      claim 2\n\n    \n    \n      if  and , then [i] and [j] do not intersect    \n\n    \n    \nWe prove by contradiction:assume then,According to , we have Since a and b are integers, we haveand since we have,which caused a contradiction, and it should be . Proved.\n\n  \n    \n      claim 3\n\n    \n    \n      Every  belongs to one of the classes \n\n    \n    \nProof:By ,  has at least n elements.By ,  has at most n elements. \n(Z/nZ,+)Q: How to “add”  and ?A:   \n\n  \n    \n      Proposition 2\n\n    \n    \n      Addtion is correctly define on  ,the class  is the same for diff. elements of  and .\n\n    \n     \nProof:\nLecture 4Warm-upFor which a,b holds:   \n\n2.For which a,b holds:\nGroups(Z/nZ,+)  is a special  element:  exists:     (generator|relations)\n(Z/nZ,)  is a special element:: not always multiplicative “inverse”Warning: , , sometimes you do have inverse.    the same but only finite primes are used.  finitely generated(just need finite generator to generate all of them)\n\n  \n    \n      Definition of Group\n\n    \n    \n      A group  is a set G and a binary operation  s.t. ,   \n\n    \n     \n Associativity:  Identity:  Inverses: PS: sometimes,  is written as  (more specific?)Q: Why “groups”?A1:  Symmetries of “objects”  it shouldn’t be only about groups!(CS: monoids:associativity+identity)Q: What to do now?A2: to deduce something from associativity+ identity+ inverses clarify something? \nHomomorphism\n  \n    \n      Definition of Homomorphism\n\n    \n    \n      A homomorphism from  to  is a map (of sets) ,s.t.\n\n    \n     \nlike \nLecture 5Warm-up1.Ans:2.Solve for  in 3.What did we cover last time in class?\nGroups:Basic Properties\n  \n    \n      Proposition\n\n    \n    \n      Identity element e is laways unique in any group G.  \n\n    \n    \nProof. By contradictionwhich caused a contradiction \n\n  \n    \n      Proposition 3.18\n\n    \n    \n       is a group. The inverse  is unique \n\n    \n    \nProof: By contradictionwhich caused a  contradiction that \n\n  \n    \n      Proposition 3.19\n\n    \n    \n      For any \n\n    \n    \nProof:  \n\n  \n    \n      Proposition 3.20\n\n    \n    \n      \n\n    \n    \nProof:\n\n  \n    \n      Cor(Prop 3.21)\n\n    \n    \n      For any  \n\n    \n   \nProof: \n\n  \n    \n      Proposition 3.22\n\n    \n    \n       \n\n    \n     \nProof:\nSubgroup is subgroup of G generated by $$\nLecture 6Warm-upHow many different ways to put  on Ans:\nSymmetric Groups\n  \n    \n      Definition of Symmetric Groups\n\n    \n    \n      Let  be a set, we define (cardinality of X=number of elements in X) is a group of all bijective maps (bijections)   \n\n    \n    \nOperation on :“composition  of functions” composition of maps\n\n  \n    \n      Proposition\n\n    \n    \n       Operation of composition (of maps) is associative Identity:  Inverses:?  \n\n    \n    \nExamples: is a finite set. has two elements.”transpositions”=maps which intercharges only tow elements. has exactly 6 elements()Q:How to think about elements of ?A_0: functions which are bijective.A_1: “matrices”,the first row: original set, the second row: image of A_2: “strings”:Example:,Q: composition via strings?“” first , second ,  A_3: cycles presentationExample:  (1 goes to the 3, and 3 goes to the 1 )(1 goes to the 3, 3 goes to the 2, and 2 goes to the 1)\n\n  \n    \n      Inverses\n\n    \n    \n      just flip it  \n\n    \n    \nLecture 7Warm-up1.Prove that  with  is a group.Proof:1.Associativity: 2.Identity:0 serves as the identity, since 3.Inverses:Every element have inverses. Inverse of  is , such that2.How to find new examples of groups?We can explore new groups by identifying subgroups of existing groups. Utilizing Existing Group Structures:By adding constraints to existing groups, such as restricting certain properties (e.g., determinant = 1 for matrices), you can form new groups. The example of  and  shows how restricting the determinant of matrices creates the special linear group.\nSubgroups\n  \n    \n      Definition of Subgroups\n\n    \n    \n       is a group, Subgroup  of  is a group  with the group operation restricted from .\n\n    \n    \nAbout restrictions of functions:  Back to Subgroups:Examples: For every :  consists of , where , is “almost the same” as  on .And  is not a subgroup od . Actually, operation is not the same(counterexamples: ).\n\n  \n    \n      Definition\n\n    \n    \n      Subgroup diagram of a group ,trival subgroup() , is contained as a set. They are all subgroups of   \n\n    \n    \n    \n\n  \n    \n      Definition of Proper Subgroups\n\n    \n    \n      Proper subgroup of  is subgroup which is not trivial or  itself.   \n\n    \n    \n\n  \n    \n      Proposition 3.30\n\n    \n    \n       is a subgroup of  iff:1.(identity of subgroup)2.(closure of the group operation)if , then ,3.(closure of inverses)if , then . \n\n    \n    \n is Problem #2  on HW#3  is a subgroup  that all of three is true  \n\n  \n    \n      Proposition 3.31\n\n    \n    \n       is a subgroup iff () and ()\n\n    \n    \nProof:(follows from 3.30): is a subgroup in :Cancellation law on the right,\nLecture 8Warm-upFind all subgroups of  \nNotationif  is a subgroup of , we write , and if  \nCyclic Subgroupsif , we have   \n\n  \n    \n      Definition\n\n    \n    \n      $Ga{a^k | k \\in Z}\\star = \\star_G$  \n\n    \n    \n\n\n  \n    \n      Lemma\n\n    \n    \n      $={a^k|Z}$ is a subgroup of G  \n\n    \n    \n\nProof. By Prop3.31if , then \n\n  \n    \n      Theorem 4.3\n\n    \n    \n      $H \\leq G \\quad s.t. \\quad  a \\in H$\n\n    \n    \nProof:  part is already proved.Let us prove the second part.Any subgroup , then $$\\forall k \\in Z, a^k \\in H, \\text{then}  \\leq H$$Therefore $HGa \\in H$\n\n  \n    \n      Definition\n\n    \n    \n      ,Order of $$ is minimal ,s.t. \n\n    \n    \nNotation:Examples: \n\n  \n    \n      Definition\n\n    \n    \n       is cyclic iff $\\exists g \\in G, G= $\n\n    \n    \nExample ? \n\n  \n    \n      Theorem 4.9\n\n    \n    \n      Every cyclic group is abelian, \n\n    \n    \nLecture 9Warm-up is defined by .Is it a homomorphism?( is all invertible  matrices with all entries from )Maybe it’s not.If  is a homomorphism:which is not true obviously.\nIsomorphism\n  \n    \n      Definition\n\n    \n    \n      Isomorphism is bijective homomorphism.(which is bijective：injective,subjective)  \n\n    \n    \nExample:  \n\n  \n    \n      Prop 11.4\n\n    \n    \n      are groups and they are homomorphism, then:1.2.3.if , then   (image of  under )3’. as , then (image of  : )4. if , then  is a subgroup of G (we call  is called pre-image of )   \n\n    \n    \nProof.1.2.3.4. Let us consider  is a subgroup?Prop 3.31: \n\n  \n    \n      Definition\n\n    \n    \n      Kernel of , Kernel is the preimage of identity: \n\n    \n    \nLecture 10Warm-up=, of G ,is it true that , ifReformulation: \nCoset\n  \n    \n      Definition\n\n    \n    \n      ,  is the coset of .  \n\n    \n    \n\n  \n    \n      Lemma6.3\n\n    \n    \n      The following are equivalent(1-3 therioticallt useful, 4 griginal warm-up, 5 How to use this lemma for actual )  \n\n    \n    \nQ: Why do we need cosets? It happens:Example:\n\n  \n    \n      Theorem 6.4\n\n    \n    \n      H-cosets partition G: we can find  a set of elements of  such that  that \n\n    \n    \nExample:Explain Theorem 6.4 in this case: what are these ?(these are equivalence classes)Reminder: originally,  or  iff The set of equivalence classes:Set of equivalence classes for the equivalence relation  iff  is denoted bt (just like )\n\n  \n    \n      Theorem 6.10\n\n    \n    \n      Lagrange’s Theorem= finite group, , (Note: all three are natural numbers)  \n\n    \n    \n\n  \n    \n      Cor 6.11\n\n    \n    \n      , (Remember, = order of =minimal  s.t. )  \n\n    \n    \nLecture 11Warm-upAre  and  isomorphic? is a cyclic group and  is not, they cannot be isomorphic.The group  has order 6, but  cannot be generated by any single element, and the orders of elements in  vary.Also,the number of elements in  and  are different.Therefore, as isomorphic groups must have the same structure and element orders,  and  cannot be isomorphic.1. is abelian,m but  is not.2. is cyclic, but  is not.3., solutions are , solutions are \nIsomorphisms\n  \n    \n      Definition\n\n    \n    \n      Isomorphism is bijective homomorphism.(which is bijective：injective,subjective)  \n\n    \n    \nExamples: HW3, Problem 1.7Q:injective?eg. provedQ:surjective? is a finite group,  of order n: ,but  for , order of  is surjective.\n\n  \n    \n      Theorem 9.7+9.8\n\n    \n    \n      If  is a cyclic group, then: if it is finite order(n), then it is isomorphic to . if it is of infinite order, then it is isomorphic to .\n\n    \n    \n\n  \n    \n      Thm/Prop\n\n    \n    \n       if  are isomorphic, then,then other is. if  are finite, then,then other is. if  are cyclic, then,then other is.\n\n    \n    \n(External) Direct ProductIdea: every “object” is easier when it’s decomposed into smaller simple pieces.  \n\n  \n    \n      Definition\n\n    \n    \n       are groups, direct product of  &amp;  is a set  with the following operation   \n\n    \n    \n  associativity: check coordinate wise.  identity:  inverses:\nLecture 12Warm-up1.What subgroups of   and  are normal?every subgroup of  is normal.() is the generator of 2.Where (0-100) the median( and average) should be for fair grading?80\nNormal Subgroups and Factor/Quotient groups\n  \n    \n      Definition\n\n    \n    \n      ,  id normal iff iff   \n\n    \n   \n\n  \n    \n      Corollary\n\n    \n    \n      if  is abelian, then every subgroup is normal.  \n\n    \n   \nProof: We have , as Q: Why do we need the definition of normalityIdea: , , we want group operation on (factor/quotient group of ):such that Problem: if How to deal with such sits? Whetheror not.\n(Remark: , it is really good!)\nFirst Isomorphism Theorem\n  \n    \n      Definition\n\n    \n    \n      if  is a homomorphism image of  kernel of   \n\n    \n   \n \n\n  \n    \n      Theorem\n\n    \n    \n      if  is a homomorphism, then ( means isomorphic,  is bijective)\n\n    \n     \nSketch of Proof:\n\n is normal, 2.To find \n\nLecture 13Warm-upProve that Proof.define a map is the inertial product of 1.injective means  or 2.surjectiveall of the elements in  can be mapped to 3.homomorphismCorrect Proof.By theorem9.27, \nDirect ProductsExternal:Internal:\n\n  \n    \n      Definition\n\n    \n    \n       is a group, ,  is the internal direct product of  and  iff1.(warning: )2.(i.e. as samll as possible)3. for any (warning: G is not abelian)   \n\n    \n   \n\n  \n    \n      Theorem\n\n    \n    \n      if  is internal direct product of , then     \n\n    \n   \n(, internal direct product, , external direct product)Proof.Let’s consider Q: is it well-defined?A:   , by 2, we know thatQ:homomorphism?+surj+inj\nLecture 14Warm-upis  finitely generated?My answer:No. Prove by contradictionif it is finitely generated, , , let Now, consider ,it cannot be generated by mutiples of  to , so it cannot be finitely generatedAns:NoHint: use induction on m  \nAbelian World\n  \n    \n      Fundamental Theorem of Finite Abelian Groups\n\n    \n    \n      Every finite abelian group  is isomorphic to , where  are primes and  are natural numbers.( is finite rank, the rest are finite piece)  \n\n    \n   \nExamples:, where , where , where , where Exercise: why? , where Proof of the theorem from Milne,Group Theory“Nonconstructive proofs”: “Assume the opposite” :$x_1 \\in G, Z/p_1^{\\alpha_1} = ,Z/p_2^{\\alpha_2} =,\\dots, Z/p_m^{\\alpha_m} ={x_1,\\ddots, x_m}{y_1,y_2 \\ldots, y_m}$\nLecture 15Warm-upWhat is a symmetry of structure? What is structure?Structure: Structure refers to the set of rules and relationships that govern the elements within a mathematical object. Structure is typically defined by the elements themselves and the operations (such as addition, multiplication) between them, along with the axioms that these operations must satisfy. For example, the structure of a group consists of a set and a binary operation defined on that set, which must satisfy associativity,identity and inverses.  \nSymmetry of structure: Symmetry refers to transformations that preserve the structure of an object. These transformations are often referred to as “automorphisms,” which are bijective mappings from the object onto itself that preserve its operations. For instance, in geometry, rotating or reflecting a square preserves its shape, and this is an example of geometric symmetry. In algebra, automorphisms of a group are symmetries of the group structure, representing permutations that preserve the group’s operation properties.\nScenario1 n people everyone votes 0 or 1 How many outcomes?A: ,n+1,2(who wins) some people know each other“some situations”=”symmetric situations”   \nLecture 16Review for Midterm I for Group TheoryGroups:abelian, non-abelianNotation: a finite set with n elements  Sets: abelian groups:non-abelian groups:\nLecture 16Warm-upwhat is  geometrically?Ans:coordinate, usually ?Then, what about Abstract Linear Algebra\nPermutation Groups is a set, , is group of all permutations(bijection ) \n\n  \n    \n      Definition\n\n    \n    \n      Permutation group G (on a set  ) is a subgroup of   \n\n    \n   \nExample:  \n\n for any \n(last Friday Reading assignment) Cube thing,  symmetries of the cube.\n\n    \n\n  \n    \n      Definition\n\n    \n    \n      .For each (stabilizer of ),  \n\n    \n   \n\n  \n    \n      Proposition\n\n    \n    \n        \n\n    \n     \nBack to Midterm: proper non-abelian subgroup of , \n\n  \n    \n      Definition\n\n    \n    \n      , Orbit of  under  is the full set   \n\n    \n   \nReminder: “ is just a bijective function”Example2: Q:What are the -orbits in ?, \n\n  \n    \n      Theorem\n\n    \n    \n      if  is finite, \n\n    \n     \nLecture 17Warm-up1.My answer:2.My answer: \nGroup Actions\n  \n    \n      Definition\n\n    \n    \n      “ acts on ” s the following map:,s.t.1.(intuition:  “acts” as id)2.,  \n\n    \n   \nExamples: , like \n\n  \n    \n      Proposition\n\n    \n    \n      “ acts on ” by bijections: if you fix any  is a bijection of X  \n\n    \n   \n is a function on Proof.bijection=injective+surjectiveClaim:  (surjective)assume , if we apply , then both  are in the image.(,) \n\n  \n    \n      Proposition\n\n    \n    \n      intuition: “ act on ” is almost the same as , if ,s.t. , then  is a homomorphism\n\n    \n   \nLecture 18Warm-up acts on (circle with 1 2 3 4 clockwise) by rotations.1.What is ?2.What is  from Def2?3.What is  from Def1?Answer:1., is “id” rotation,i.e. does nothing.Choice:  “acts by” rotation,. but also the picture suggests additional structure of “circular order”. i.e. a circle with 1,2,3,4 clockwise.2., so Def.2: acts on \nGroup Actions\n  \n    \n      Proposition\n\n    \n    \n      if  with 1 and 2, then () is homomorphism  \n\n    \n     \nBy First Isomorphism Theorem for (for ): , where here  is the kernel of action Back to Warm-up:3.,   \n\n  \n    \n      Definition\n\n    \n    \n       as a set, .(Left regular action of G on itself)\n\n    \n    \n\n  \n    \n      Theorem9.12\n\n    \n    \n      Every group is isomorphic to a group of permutations\n\n    \n    \nSketch of proof:(Same as for )\nLecture 19Warm-upHow many different colorings of a square are there?My answer:6 or 16\nBurnsides’s Lemma\n  \n    \n      Theorem\n\n    \n    \n       is a finite group,  is a finite set,  acts on (“” is a -set), Then number of -orbits on ,where   \n\n    \n    \nRemark:  all points fixed by whole .Problem: let’s consider the same square anda:2013 colorsb:201320132013 colorsHow many different coloring do we have?if two colorings are different by isometry(rigid motions=rotations+reflections) of the square, they’re same.Idea:Let’s consider situation with   colors, \n\n\n\nHeader1\nHeader 2\nHeader3\nHeader 4\n\n\n\nid\nHeader 2\nH\nHeader 4\n\n\n\nn\nV\n\n\n\n\nRow 2\nZ\n\n\n\n\nRow 3\nY\n\n\n\nX=Set of al possible colorings of the square with  colorsall: so the -orbits in X is the number of different colorings in  colors all coloring stable under  -rotation\nLecture 20Warm-upFunctions-why?How?compute: \nMotivation for Rings and FieldsIdea: Sometimes we need arithmetic not only with .\n\n\n\nHeader1\n\n\n\n\n\n\n\n\nSet\n\n\n\n\n\n\n\n+/-\n1+1=2,2-3=-1\n\n1.2+1.2=2.4\nusual addition\naddition of matrices\n\n\nMultiplication\n\n\n\nyou disturbute them\nmultiplication of matrices\n\n\nDivision\nalmost never\nsometimes\nalmost always\ndivisor of polynomials\n?not sure\n\n\nDivision Algorithm\n\n\n\ndivision algorithm for polynomials\nDo Not commute\n\n\nDivisors\n\n?\n/\ndivisor algorithm for polynomials\n\n\n\nPrime Numbers/gcd\n\n?(ideals)\n/\nIrreducible polynomials\n\n\n\nEuclidean Algorithm\ngcd(a,b)=ca+db\n?(ideals)\n/\nEuclidean algo for polynomials\n\n\n\nLecture 21Last LectureArithmetic,sets+operationspolynomials:division of polynomials.some results from  are generalizable.  \nHW.Problem1 is generated by transpositions \nRings and Fields\n  \n    \n      Definition\n\n    \n    \n      Ring is a set  with two operations , with 1-6 axioms from the book .Reformulation: is an abelian(1) group(2,3,4). is associative(5).(+ unit for “”) distributivity:(6) identity for “+”:0, unit for “”:1.(7):  is commutative: commutative rings with 1.(8)  \n\n    \n    \nExample:: yes: yes,:Yes: yes: yes,:YesQ: -inverses?for , only  and  are invertible. Answer for the question(-inverses?) for any =greoup of units.(-inversible element of )for , non-zero numbers are invertible.  \n\n  \n    \n      Definition\n\n    \n    \n      Field is a ring  with  and ,i.e. if  and , then .  \n\n    \n   \nLecture 22Warm-upunits of  form a group,   find    \n\n  \n    \n      Definition\n\n    \n    \n      A unit  i an element s.t.    \n\n    \n      \n\n  \n    \n      Definition\n\n    \n    \n       Gaussian integers, namely   \n\n    \n    \n, because \n\n  \n    \n      Definition\n\n    \n    \n      A Zero divisor is an element a s.t.   \n\n    \n    \n\n  \n    \n      Proposition\n\n    \n    \n      For any ring R, \n\n    \n    \nProof,on the contrary, Goal:  describe “types” of elements in ,  Units of   Zerodivs of   else\n\n\n\nR\nUnit of R\nZerodivs of R\nelse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n (general phenomenon)\n\n\n\n  \n    \n      Proposition 16.15\n\n    \n    \n      If , then (Cancellation law of non-zerodivisors)  \n\n    \n    \n\n  \n    \n      Theorem 16.16\n\n    \n    \n       is finite and integral domain, then  is field. \n\n    \n     \nfinite   Units of , nothing else.integral domain field  Unites of  =Exer:  is a finite group, , order of  is finite, \nLecture 23 units of  ZeroDivisors of  elseQuestion: find a ring  with all three nonemptyMy answer::  \n\ndet(M)=1  invertible  \n0  \nnon-zero but not invertible\n\nAnswer: - non commutative Direct Product:, eg., take instead \nDivision Algorithm for Integral Domains\n  \n    \n      Theorem 1.1\n\n    \n    \n      For any , there exist a pair , , where   \n\n    \n    \neg. 51=6*8+3(a=bq+r)  \n\n  \n    \n      Theorem 1.2\n\n    \n    \n      For any  there exists ,where   \n\n    \n     \nExample. ,,   \n\n  \n    \n      Cor(Euclidean Algorithm)\n\n    \n    \n      You can compute gcd of 2 polynomials using Thm1.2  \n\n    \n    \nGaussian Integers\n  \n    \n      Theorem 1.?\n\n    \n    \n      for , For any , there exist , where \n\n    \n    \nexample:, find at least 4 pairs of \nLecture 24Homomorphisms and Ideals  homomorphisms( means subgroup,  means normal subgroup)First Isomorphism Theorem: First Isomorphism Theorem: ( isomorphism of rings)  \n\n  \n    \n      Definition\n\n    \n    \n      Isomorphism  is bijective homomorphism( of rings)\n\n    \n    \nExamples:Prove that  is homomorphism.Proof. is surjective  \n\n  \n    \n      Definition\n\n    \n    \n      (analog of a normal subgroup in Group Theory)Ideal:   in  isa.additive subgroupb.   \n\n    \n   \nIntuition: is a homomorphism of additive groups  and Therefore , is a normal subgroup of By axioms,  is abelian. So any subgroup in  is normalfor b:Fact: . Therefore, if , then Q for First Isomorphism Theorem for rings: what is ? What is  ?  \n\n  \n    \n      Definition\n\n    \n    \n      A quotient of  by  is a set  pf dements  \n\n    \n    \n(12)(45)(37)LCM(2,3)=6,LCM(2,3,4)=12,LCM(2,5,8,10)=40\nLecture 25Warm-upprove thatare rings but not fields.My answer:  \n\nabelian group  \nassociative for   \ndistributivity  \nidentity for “+” and units for “”\n\nAnswer:1.but it is not field( is not  invertible)2.Z[-5]\nZ[-5]- integral domain\n  \n    \n      Lemma\n\n    \n    \n       is an integral domain\n\n    \n   \nthere no Proof1.Assume there exists Proof2., no zero divisors in  as no zero divisors in  \n units  else\n\n  \n    \n      Lemma\n\n    \n    \n       is not a unit in \n\n    \n     \nProof.Opposite: Geometric idea:, not possible \n\n  \n    \n      Proposition\n\n    \n    \n        \n\n    \n     \nUnits in  are only 1 and -1 because   if there exists  s.t. , then   is integerso we have   \n\n  \n    \n      Definition\n\n    \n    \n      (  else of ) is prime iff if  then  or .  \n\n    \n     \n\n\n  \n    \n      Definition\n\n    \n    \n      ( else of ) is irreducible iff , then either  or  is a unit.\n\n    \n   \n(only divisors of  are units and p itself)\n\n  \n    \n      Lemma\n\n    \n    \n      3 is not a prime in 3 is irreducible in   \n\n    \n   \nProof., but , if one of them is 1, this is what we claimed.Then we assume  there is no solution\nLecture 26Burnside’s Lemma= finite group, = finite set and  acts on , so number of -orbits on = , where  number of elements , such that  fixes ,. \nProblem1 symmetric gtoup on 4 elements(),,Q:How many orbits? via Burnside’s Lemma.$\\frac{1}{24}(4+62+81)=1$\nProblem2 the same group as Problem1,  a set of pairs  with  ( is allowed)Q:How many orbits? via Burnside’s Lemma$\\frac{1}{24}(16+64+81)=2$\nLecture 27Homomorphisms and Ideals\n\n\n\nGroup Theory\nRing Theory\n\n\n\nFirst Homomorphism Theorem\n\n\n\n\nWhat is this isomorphism\n\n\n\n\nOperation\n\n\n\n\nWhy do I talk about equivalent classes\n set of equivalent classes for  iff \n iff \n\n\nfunctions\n is isomorphism of groups(1.homomorphism of groups, and 2.inj+surj=bij)\n with homomorphism of rings plus bijiective= isomorphism of rings\n\n\nCorollary:Canonical Decomposition of Homomorphisms\n\n\n\n\n\n{Kernels of homomorphisms of groups}={normal subgroups}\n{Kernels of homomorphisms}={ideals}\n\n\n\n  \n    \n      Proposition\n\n    \n    \n        \n\n    \n     \n\n  \n    \n      Proposition\n\n    \n    \n        \n\n    \n     \n\nwhat is happening here?1. is trivial, 2.  is injective\nLecture 28Warm-upIs  an ideal of (ring of integers) and (polynomials in x with Z coefficient) is an ideal of , not an ideal of .Ans:  is not an ideal of , because \nHomomorphisms and Ideals IIIReminder:First Isomorphism Theorem for rings:fix Question for 11/13:When  a field?Question for 11/15:When  a integral domain?  \n\n  \n    \n      Proposition\n\n    \n    \n       is a field(), then the only ideals in  are R$  \n\n    \n       \nProof. Let’s if  and it’s the only element: if , then Reminder: from definition of ideal, , so we havefrom Preliminary Problem 1, if ( )so we have .proved  \n\n  \n    \n      Definition\n\n    \n    \n       is a maximal ideal in , there are no \n\n    \n    \n\n  \n    \n      Theorem 16.35\n\n    \n    \n       is a commutative ring with , ,  is a field iff  is a maximal ideal in .  \n\n    \n      \nIdea: the key tool is Second Isomorphism Theorem.  \n\n  \n    \n      Second Isomorphism Theorem(16.32)\n\n    \n    \n      if , then    \n\n    \n      \nSo the field  no proper ideals and proposition above.Example:   \n\n  \n    \n      Proposition\n\n    \n    \n       is maximal\n\n    \n    \nProof. opposite , pick , by Euclidean Algorithm, for \nLecture 29Warm-up1.Why  in ?2.What is  in ?3.If  is a ring with only ideals  and , then  is a field.My Answer:1.2. in  is , or3.Step1： Take , consider  then Step2: As  then , proved.  \nHomomorphisms and Ideals IVReminder:   \n\n  \n    \n      Lemma\n\n    \n    \n       is an integral domain iff  is a prime ideal.\n\n    \n    \n\n  \n    \n      Definition\n\n    \n    \n       is prime iff  then  or   \n\n    \n    \nProof of the Lemma: either  or \n\n  \n    \n      Theorem16.38\n\n    \n    \n      = commutative ring with , then  is integral domain  is prime.   \n\n    \n    \nProof: $[0]{R/I}=I=0_R+I[a]{R/I} \\cdot [b]{R/I}=[0]{R/I} \\Rightarrow [a]=[0] or [b]=[0], I \\Leftrightarrow a \\in Ia \\cdot b \\in I \\Rightarrow a \\in Ib \\in I$\n\n  \n    \n      Corollary\n\n    \n    \n      Every maximal ideal is prime. However, in general, \n\n    \n    \nExample: Last time:  is maximal,\n\n  \n    \n      Lemma\n\n    \n    \n       is prime  \n\n    \n    \n or  or  or , proved\nLecture 30Warm-up1.Find all ideals  s.t. 2.Find all ideals  s.t. 3.Why  is an ideal?Ans:\n\n   \n  \nadditive subgroup and , where ideals in   graph TD;\n id2[\"(2)\"] --&gt; id6[\"(6)\"];\n id2[\"(2)\"] --&gt; id10[\"(10)\"];\n id3[\"(3)\"] --&gt; id6[\"(6)\"];\n id3[\"(3)\"] --&gt; id15[\"(15)\"];\n id5[\"(5)\"] --&gt; id10[\"(10)\"];\n id5[\"(5)\"] --&gt; id15[\"(15)\"];\n id6[\"(6)\"] --&gt; id30[\"(30)\"];\n id10[\"(10)\"] --&gt; id30[\"(30)\"];\n id15[\"(15)\"] --&gt; id30[\"(30)\"];\n\nHomomorphisms and Ideals V\n  \n    \n      Definition\n\n    \n    \n       commutative ring with , $(x)=={r \\cdot x | r \\in R}$ is principal ideal generated by x.\n\n    \n    \nRemark: if in , all ideals are principal(and  is integral domain), then  is called PID=principal ideal domain.\n\n  \n    \n      Definition\n\n    \n    \n       commutative ring with  ideal generated by \n\n    \n    \nExample:   \n\n  \n    \n      Proposition\n\n    \n    \n      a. is an ideal.b. is a minimal ideal  such that \n\n    \n    \nproof. 2 for a: 1 for a :   \n\n  \n    \n      Definition\n\n    \n    \n       commutative ring with I,J \\unlhd R, I \\cdot J={i_1 \\cdot j_1+\\ldots i_m \\cdot j_m| i_1 \\ldots i_m \\in I, j_1,\\ldots j_m \\in J }$, this is multiplication of ideals.\n\n    \n    \nNotice that  ideal generated by , here,  is fixed.However, here the definition of multiplication of ideals,  is not fixed.\n\n  \n    \n      Proposition\n\n    \n    \n      \n$ \\cdot =$  \n  \n\n\n\n    \n    \nLecture 31Warm-upwhy ideals are not scary?, we know  for any .1.Find 2. When $ \\subseteq &lt;b,c&gt;?&lt;b,c&gt; \\subseteq $Ans:  \n\n, the only condition is   \n for some \n for some \n\nArithmLecture 28:toward gnosisLecture 29:kairos of arithmetic  \n\nHw10: Arithmetic EpiphonyLecture 30: Arithmetic in Rings V and Arithmetic Metanoia\n\nLecture 2/09.04 Fundamental Theorem of Arithmetic in Z, , where  are primesLecture 20/10.21Arithmetic in Rings: Division Algorithms and Euclidean Algorithm lead to Fundamental Theorem of ArithmeticLecture 22/10.25Units and Zero-Divisors: 1. Why do arithmetic on “else”.2.Units stand “in front “in Fundamental Theorem or ArithmeticLecture 23/10.27Fundamental Theorem of Arithmetic for  are primeLecture 25/11.01 1. prime elements and ireeducible elements.2.no FUndamental Theorem of Arithmetic in Lecture 29/11.18 and Hw10$a\\cdot c=c \\leftrightarrow \\cdot=\\forall \\text{ideal} I \\unlhd Z[\\sqrt{-5}], \\exists a_1,a_2 \\ldots a_m:I=p_1^{a_1}\\ldots p_m^{a_m}p_i\\forall (n) \\unlhd Z, \\exists a, s.t. (n)=(p_1)^{a_1} \\ldots (p_m)^{a_m}, (p_i)$ are prime ideals \nLecture 32Vector Spaces or Linear Algebra in Abstract Algebra\n  \n    \n      Definition\n\n    \n    \n      A vector space  over F is:  \n\nan abelian group  \na multiplication by scalar\n\nhere,  field = Double subscripts: use braces to clarify _ \\cdot _:F \\times V \\rightarrow V s.t.:  \n\n  \n  \n  \n\n\n\n    \n      \n “F”-actionExamples:  “algebraically closed”()\n\n  \n    \n      Definition\n\n    \n    \n       is a linear map,  are -vector spaces maps from  to (matrices):  \n\nhomomorphism of abelian groups  \n\n\n\n    \n     \n\n\n  \n    \n      Definition\n\n    \n    \n      A basis of  of  is a set of vectors  s.t.  \n\n  \n are linearly independent.\n\n\n    \n    \n\n in basis .   \nfield,   \n\n is a vector space over .  \nFind basis for \n\n  \nLecture 34Warm-up1.Find multiplicative inverses for  in 2.Find multiplicative inverses for  in 3.Find multiplicative inverses for  in  but as in 2  \n\n  \n\n\n\n\nFields I\n  \n    \n      Definition\n\n    \n    \n      Field F is 1-8 s.t.  and  has multiplicative inverses.\n\n    \n  \nExamples: for  is a prime,, like ,also   \n\n  \n    \n      Definition\n\n    \n    \n      Extension of fields:  is a subfield of K\n\n    \n  \nExample: subfield=subset+same structuresusually drawn as (from smallest to largest, this arrow contains injective homomorphisms) is the same thing.\n\n  \n    \n      Lemma\n\n    \n    \n       is a field extension , then  is a vector space over .\n\n    \n    \n\n\nabelian group\nscalar multiplicationdegree of   \n  \n    \n      Theorem 21.17\n\n    \n    \n      ,   \n\n    \n    \n is a field(vector space over Q) , but  where \n\nLecture 35Warm-upFind a minimal polynomial forDo you remember about minimal/characteristic polynomials in Linear Algebra?My ans: is minimalfrom preliminary problem 1 from last lecture,  but  then  as ()like However, like , it is wrong since .if , it is not true.  \nField I |  (finite extension)(this = means as  vector space)let , degree of So let us choose  basis: |(as Q-vector space), Any   \n\n  \n    \n      Lemma\n\n    \n    \n       is a linear map of E-vector space.  \n\n    \n    \nRemark: is not multiplication by scalar. As “Scalar” for E-vector spaces is an element of E, not F. is “multiplication by a scalar”.“Proof:” , distributivity,commutativity.Q1: what is  in basis  (in )?\n\n  \n    \n      Lemma\n\n    \n    \n       is invertible.(“matrix  is invertible”)\n\n    \n     \nReformulation:\nLecture 36Field II, it is the minimal polynomial. is a field?, here  is a ring for polynomials(invariable x), with  coefficients. is an ideal in  generated by a single element ,   \n\n  \n    \n      Proposition\n\n    \n    \n         \n\n    \n      \nWe know that  is a field iff  is maximal.Q:How did we prove maximality of ?79A: Assume , s.t. , take  but not in .  as  is irreducible in assume By Euclidean Algorithm for , where , which caused a contradiction that .So  is maximal.  \n\n  \n    \n      Theorem\n\n    \n    \n       is a field,  is irreducible, then  is a field. \n\n    \n    \nWhy would this be useful?Example0:   is a field.Example: is a field.\n","categories":["Math113-Abstract Algebra"]},{"title":"CS188 Cheatsheet","url":"/2024/12/12/CS188-Final-Pack/","content":"\n\n  6ac18c6ed26c5158934bd440f4f062ac994d58f86750b4eed4eaa57f3a408cddb9bb19e1fdee0eea5bb31a5ab9dff254d55a8f9f9d4d9ef2f1efdad3771acdb5f2605f112cd92267b2aa33668a1267f02b3c96a3f92510d1936c5ed830ac805be3f8c3cd8e0eb447c6ea2b0b94a5cebcb4f4eb9e17147584c87b15f98029ab47ee7e1eadbbc83af3bc3374af3db704a3fb41b104991a731911cb7093224151487afb9764685d527e020f039bcb521f237ea5d840da6819d80508a7297f2b7beb5517fddcaeff5765d50c03151449977e95f38cb20d8737fe4a8128c60b0d199d1ba8fc2f3f07424fbb6a8461fef1792f7f0051378e030e5d71da19c58fbf44ec8b4ca9fffa171eedb2dd2fb72fc6de70892443604f64686eaa123e013103452ca41d8203be216b81a1ea7ab11ca018054682bac5e9d22aaf61361dbab0943c79bac0e28e2165100e5010c255d4b3f5808bd7a556b709dc835de193afd69d20e288acc61c8d720885c187827ae1a5193504b981c195b1045b6070dd00f5ad0e5c0bc49a279089f025d2f3ef7cdd88a884cacd70afa21bcc9719f09cdd3acdce9d20aeb086659601c5a27c4e5eee838f4f2372f8cac9be558c7e10ef325a046fa8b90e80e7082eb417a85c87097728535471a8065f8380554965ece6fe36a52ded\n  \n    \n      \n      \n        Hey, password is required here.\n      \n    \n  \n\n","categories":["Introduction to Artificial Intelligence"]},{"title":"(CS182)Machine Learning Review","url":"/2024/06/04/Machine-Learning-Review/","content":"Scope of Final Exam:Bayes’ Decision Rule, Discriminant Functions:Maximum Likelihood Estimation, Bayesian Estimation,   Paramatric Classification, Model Selection:Paramatric Classification Revisited:Multilayer Perceptrons:Hard-Margin SVM, Soft-Margin SVM, Kernel Extension:PCA, LDA:k-Means Clustering Algorithm, EM algorithm, Use of Clustering:Nonparametric Density Estimation, Nonparametric Classification:CNN, RNN:Voting, BoostingCross-Validation, Interval Estimation, Performance Evaluation\nPreliminariesDistance MeasuresDistance between instance  \nDistance between groups  \nLinear AlgebraTransposeMatrix  is orthogona if  and \nInverse\nGradient Vector\nPositive Semidefinite MatricesA symmetric matrix  is said to be:positive semidefinite:  for all positive definite:  for all indefinite: both  and   are not positive semidefiniteIf  and  is , with , then  is positive definite. If , then  is positive semidefinite.A positive definite matrix can be “factored” as ,where  is a nonsingular upper triangular matrix. One way to obtain  is by Cholesky decomposition.\nEigendecomposition(Spectral Decomposition)where Q is a square  matrix whose columns are the eigenvectors of  ordered in terms of decreasing eigenvalues, and  is a diagonal  matrix whose diagonal elements are the corresponding eigenvalues.If  is symmetric, its eigenvectors are mutually orthogonal and the eigenvalues are real. In this case, it can be written as\nSingular Value DecompositionIf  is , it can be written aswhere  is  and contains the orthonormal eigenvectors of  in its columns,  is  and contains the orthonormal eigenvectors of  in its columns, and the  matrix  contains the  singular values,  on its diagonals that are the square roots of the nonzero eigenvaluesof both  and  ; the rest of  is zero.where  and   are of different sizes but are both square and contain ontheir diagonal and 0 elsewhere.\nOptimization PrimerLagrange multipliers can transfer a problem with  variables and  constraints to a problem that has  variables and no constrains.Standard foem problem:LagrangianAssume we do not consider the constraints and obtain the optimal value in point , we have 3 situations.For the first situation, the constrains is ignored and satiesfied automatically. For the second situation, we can obtain the answer by simply calculate the Lagrangian. For the third situation, we should discard the answer. And in conclusion, if , the problem can be transformed into a problem without constraint and if , we can use Lagrange multiplier. And in conclusion, And we have  Above all, we can conclude the Karush-Kuhn-Tucker condition:   \nBayesian Desicion ModelParameter EstimationLinear DiscriminationMultilayer PerceptionsSupport Vector MachinesHard-Margin Support Vector MachineDefinition of hyperplaneDefinition of distanceIf the sample point can be classified properly, we haveNearest points to the hyperplane are called support vector, and the sum of distance of two support vectors to the hyperplane from different class is called marginWe want to maximize the distance, or equivalently, minimize , we have\nDual ProblemWe have its lagrangian formCalculate its derivative and we haveAnd we can have its dual formIt should satisfy KKT conditionsAnd if we have  calculated, we havewhere its optimal will be\nSoft-Margin Support Vector MachineRelaxed seperation constriantsPrimal optimization problemIts lagrangian can be written asDual problemAnd its KKT conditions are\nkernel extensionInstead of defining a nonlinear model in the original (input) space, the problem is mapped to a new (feature) space by performing a nonlinear transformation using suitably chosen basis functions.\n\nSimilarly, we haveIts dual problem isSince calculating  is difficult, we can use kernel trick to avoid this problem(calculate its value in original space)And we can rewrite the dual problem asAnd the answer isPolynomial kernel:\n\nwhere ( q ) is the degree.\nE.g., when ( q = 2 ) and ( d = 2 ),\n\nwhich corresponds to the inner product of the basis function\n\nWhen ( q = 1 ), we have the linear kernel corresponding to the original formulation.\nSupport Vector RegressionMinimization Problem:\n$$\\text{minimize}{\\mathbf{w}, w_0} \\quad \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_t \\left( |r^t - f(\\mathbf{x}^t)| - \\epsilon \\right)+$$\nPrimal optimization problem:\n\nLagrangian:\n$$\\begin{aligned}\\mathcal{L}(\\mathbf{w}, w_0, {\\xi^+_t}, {\\xi^-_t}, {\\alpha^+_t}, {\\alpha^-_t}, {\\mu^+_t}, {\\mu^-_t}) = \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_t (\\xi^+_t + \\xi^-_t) \\\n\n\\sum_t \\alpha^+_t \\left[ \\epsilon + \\xi^+_t - r^t + (\\mathbf{w}^T \\mathbf{x}^t + w_0) \\right] \\\n\\sum_t \\alpha^-_t \\left[ \\epsilon + \\xi^-_t + r^t - (\\mathbf{w}^T \\mathbf{x}^t + w_0) \\right] \\\n\\sum_t (\\mu^+_t \\xi^+_t + \\mu^-_t \\xi^-_t)\\end{aligned}$$\n\nDual optimization problem:\n  \n\n\n\nFunction Representation:\n\nDimentionally ReductionPrincipal Component AnalysisWe aim to fina a linear mapping from d-dimensional input space to k-dimensional with minimum information loss according to some criterion(k&lt;&lt;d)We have the scalar projection of  on the direction  s.t. we want to find the component that can make the sample points the most spread out, which simply can be achieved by maximize its variance,And the goal of PCA will beand we can do eigenvalue decomposition to covairencematrix , and sort the eigenvalue. The answer to PCA is the front  eigen vectors And for choosing the detalied number of k, we can have a threshold,i.e. .Then, we use this formula to choose the minimal ,\nFactor AnalysisThe target of FA is opposite to that of PCA:PCA (from x to z):A (from z to x – generative model):\nMultiple Dimensional ScalingWe want to embed the points in a lower-dimensional space (e.g., two-dimensional space) such that the pairwise Euclidean distances in this space are as close as possible to those in the original space.Assuming m sample point in original space have a distance matrix , we aim at get representation of sample in  dimensional space , s.t. (disntance in -dimensional space equals that in original space)Let ,  is the inner product of sample after dimensional reduction, we haveCentering of data to constrain the solution:And it is obvious that the sum of the row and column of  is zero, thenLet’s have$$dist_{i*}^2=\\frac{1}{m} \\sum_{j=1}^m dist_{ij}^2\\quad dist_{j}^2=\\frac{1}{m} \\sum_{i=1}^m dist_{ij}^2 \\quad dist_{i}^2=\\frac{1}{m^2} \\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2b_{ij}=\\frac{1}{2} (b_{ii}+b_{jj}-dist_{ij}^2)=-\\frac{1}{2}(dost_{ij}^2-dist_{i*}^2-dist_{j}^2+dist_{**}^2)B=V \\Lambda V^T$$And let $\\Lambda_{}d’V_{}$Z=\\Lambda_{}^{1/2}V_{*}^T \\in \\mathbb{R}^{d’ \\times m}$$\nLinear Discriminant AnalysisGoal: the classes are well-separated after projecting to a low-dimensional space byutilizing the label information (output information).Let  be the set, mean vector, covariance matrix of -th sample set. We have the center of each class after projection , and sample covariance .We want to minimize covariance between in-class covarianceAlso, we want to maximize the distance between center of classes$$ ||w^T \\mu_0- w^T \\mu_1||{2}^{2}  J=\\frac{||w^T \\mu_0- w^T \\mu_1||2^2}{w^T \\Sigma_0 w+ w^T \\Sigma_1 w}=\\frac{w^T (\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T w}{w^T \\Sigma_0 w+ w^T \\Sigma_1 w} S_w=\\Sigma_0+\\Sigma_1=\\sum{x\\in X_0}(x-\\mu_0)(x-\\mu_0)^T+\\sum{x\\in X_1}(x-\\mu_1(x-\\mu_1)^T )S_b=(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^TJ=\\frac{w^T S_b w}{w^T S_w w}min_w -w^T S_b w \\quad s.t. \\quad w^T S_w w=1\\lambda(\\mu_0-\\mu_1)=S_b w= \\lambda S_w w \\Rightarrow w=S_w^{-1} (\\mu_0-\\mu_1)S_t=S_b+S_w=\\sum_{i=1}^m (x_i-\\mu)(x_i-\\mu)^T, \\quad S_w=\\sum_{i=1}^N S_{w_i}, \\quad S_b=S_t-S_w=\\sum_{i=1}^N m_i(\\mu_i-\\mu)(\\mu_i-\\mu)^Tmax_w \\frac{tr(W^T S_b W)}{W^T S_w W}$$the solution is the eigen vector relevant to -th maximal non-zero eigenvalue of the matrix \nClustering and Mixture ModelsIntroductionMixture(density) model=mixture components(clusters or groups)=component densities=mixture proportions (priors)\nGaussian Mixture Model(GMM)  parameters \nK-Means Clustering AlgorithmEncoding: from data point  to the index  pf a reference vectorDecoding: from an index  to the corresponding reference vector Each data point  is represented by the index  of the nearest refernce vector:Total recontruction errorwhere if  and 0 otherwise. \nExpectation-Maximization AlgorithmFor unobserved data, we can use EM to do estimation. We call this unobserved variables .Let  be observed variables, and  represent latent variable and  be model parameters, we haveSince Z is latent variable, it can not be optimized directly, we can calculate the expectation of  to maximize marginal likelihood for observed variable iteratively.\nE-StepInfer the distribution of latent variables  by current parameter  and calculate its log likelihood for the expectation of \nM-StepFind parameter  that maximize expectation likelihood, namelyIn one word, EM calculate global optimal by caulate expectation of current parameter and find the parameter that can maximize likelihood expectation.\nUse of ClusteringData ExplorationLike dimensionality reduction methods, clustering can be used for data exploration:Dimensionality reduction methods: find correlations between features (and thus group features). Clustering methods: find similarities between instances (and thus group instances).Clustering allows knowledge extraction through:Number of clusters,Prior probabilities,Cluster parameters\nClustering as PreprocessingAfter clustering, the estimated group labels may be seen as the dimensions of a new k-dimensional space.\nMixture of Mixtures in ClassificationIn classification, when each class is a mixture model composed of a number ofcomponents, the whole density is a mixture of mixtureswiths\nHierarchical Clustering  \nNonparametric MethodsDeep Learning ModelsDeep AutoencodersAn autoencoder (AE) is a feedforward neural network for learning a compressed representation or latent representation (encoding) of the input data by learning to predict the input itself in the output.The hidden layer in the middle is constrained to be a narrow bottleneck (withfewer units than the input and output layers) to ensure that the hidden unitscapture the most relevant aspects of the data.\nStacked Denoised AutoendcoderA denoising autoencoder solves the following (regularized) optimization problem:\n$$\\text{minimize}_{\\mathbf{W}1, \\mathbf{W}2, \\mathbf{w}{01}, \\mathbf{w}{02}} \\quad \\frac{1}{2} \\sum_\\ell | \\mathbf{x}^\\ell - \\hat{\\mathbf{x}}^\\ell |_2^2 + \\lambda \\left( | \\mathbf{W}_1 |_F^2 + | \\mathbf{W}_2 |_F^2 \\right)$$\nwhere\n$$\\begin{aligned}    \\mathbf{h}^\\ell &amp;= \\sigma(\\mathbf{W}1 \\tilde{\\mathbf{x}}^\\ell + \\mathbf{w}{01}) \\    \\hat{\\mathbf{x}}^\\ell &amp;= \\sigma(\\mathbf{W}2 \\mathbf{h}^\\ell + \\mathbf{w}{02})\\end{aligned}$$\nHere  is a regularization parameter and  is a matrix norm, called the Frobenius norm, which is defined as\n$$| \\mathbf{A} |F^2 = \\sum{i,j} a_{ij}^2 \\quad \\text{where} \\quad \\mathbf{A} = [a_{ij}] \\text{ is a matrix}$$Stacked denoising autoencoders can be formed by stacking the denoisingautoencoders in a layer-wise manner like deep autoencoders with the samepretraining, unrolling, and fine-tuning steps.\nConvolutional Neural NetworksDropout is very powerful because it effectively trains and evaluates a bagged ensemble of exponentially many deep models which are sub-networks formed by removing (randomly with a dropout rate of say 0.5) units or weights (achieved simply via multiplying their values by zero) from the original CNN. \nRecurrent Neural NetworksRecurrent neural networks (RNNs) are extensions of feedforward neural networks for handling sequential data (such as sound, time series (sensor) data, or written natural language) typically involving variable-length input or output sequences.There is weight sharing in an RNN such that the weights are shared across different instances of the units corresponding to different time steps.A classical dynamic systemA more generalized form A recurrent neuron with a scalar output  A layer of recurrent neurons with a vector output  \n\n\n\nThe following gradients (used by gradient-based techniques) can be computedrecursively (details ignored) like the original backpropagation algorithm:\nRNN GeneralizationsEncoder-Decoder RNN  And there are many ways to increase the depth  Dependencies between Events in RNNs    Shortcomings of long-term RNN:1. vanish or explode of gradients.2. computationally intensiveLSTM is based on the idea of leaky units which allow an RNN to accumulateinformation over a longer duration (e.g., we use a leaky unit to accumulate evidence for each subsequence inside a sequence).I Once the information accumulated is used (e.g., suffcient evidence has been accumulated for a subsequence), we need a mechanism to forget the old state by setting it to zero and starting to count again from scratch.            Total RNN \nEnsemble LearningVoting\nBoosting   \nModel Assessment SelectionCross-Validation and ResamplingK-Fold Cross ValidationThe dataset  is randomly partitioned into  equal-sized subsets , , called folds.\n training/validation set pairs:\n$$\\begin{aligned}\\mathcal{T}_1 &amp;= \\mathcal{X}_2 \\cup \\mathcal{X}_3 \\cup \\cdots \\cup \\mathcal{X}_K &amp; \\mathcal{V}_1 &amp;= \\mathcal{X}_1 \\\\mathcal{T}_2 &amp;= \\mathcal{X}_1 \\cup \\mathcal{X}_3 \\cup \\cdots \\cup \\mathcal{X}_K &amp; \\mathcal{V}_2 &amp;= \\mathcal{X}_2 \\&amp; \\vdots &amp; &amp; \\vdots \\\\mathcal{T}_K &amp;= \\mathcal{X}_1 \\cup \\mathcal{X}2 \\cup \\cdots \\cup \\mathcal{X}{K-1} &amp; \\mathcal{V}_K &amp;= \\mathcal{X}_K \\\\end{aligned}$$\nAny two training sets  and  () share  folds.\n5 × 2-Fold Cross-ValidationFor each iteration, the dataset  is randomly split into two equal-sized parts,  and , which leads to a 2-fold cross-validation. With 5 iterations, we get  training/validation set pairs:\n$$\\begin{aligned}\\mathcal{T}_1 &amp;= \\mathcal{X}_1^{(1)} &amp; \\mathcal{V}_1 &amp;= \\mathcal{X}_1^{(2)} \\\\mathcal{T}_2 &amp;= \\mathcal{X}_1^{(2)} &amp; \\mathcal{V}_2 &amp;= \\mathcal{X}_1^{(1)} \\\\mathcal{T}_3 &amp;= \\mathcal{X}_2^{(1)} &amp; \\mathcal{V}_3 &amp;= \\mathcal{X}_2^{(2)} \\\\mathcal{T}_4 &amp;= \\mathcal{X}_2^{(2)} &amp; \\mathcal{V}_4 &amp;= \\mathcal{X}_2^{(1)} \\&amp; \\vdots &amp; &amp; \\vdots \\\\mathcal{T}_9 &amp;= \\mathcal{X}_5^{(1)} &amp; \\mathcal{V}_9 &amp;= \\mathcal{X}5^{(2)} \\\\mathcal{T}{10} &amp;= \\mathcal{X}5^{(2)} &amp; \\mathcal{V}{10} &amp;= \\mathcal{X}_5^{(1)}\\end{aligned}$$In total, we have done 5 times of 2-fold cross-validations.  \nInterval EstimationTwo-sided confidence intervalorOne-sided confidence interval\nand\n\nStudents’ t-distriburionIn the previous intervals, we used (\\sigma); that is, we assumed the variance is known. When the variance (\\sigma^2) is not known, it can be replaced by the sample variance\n\nThe statistic  follows a t-distribution with (N - 1) degrees of freedom: (long proof omitted here …)\n\nFor any  we have used the symmetry around 0 of the t-distribution, i.e., ,\n\nor\n\nHypothesis TestingWe can also define a one-sided test  or one-tailed test.Null and alternative hypotheses:\n\n\nas opposed to the two-sided test when the alternative hypothesis is (\\mu \\neq \\mu_0).The one-sided test with level of significance  (\\alpha) defines a 100(1 - (\\alpha)) confidence interval bounded on one side in which (m) should lie for the hypothesis not to be rejected.We fail to reject (H_0) with level of significance  if\n\nOtherwise, we reject  with region of rejection .If the variance  is not known, the sample variance  will be used instead and so\n\nE.g., we can define a two-sided t test :\n\n\nWe fail to reject at significance level if\n\nOne-sided t test  can be defined similarly.\nPerformance EvaluationTest statistic: \nwhere\n \n \n \nWe can define a one-sided t test :\n \n \nWe fail to reject at significance level  if\n\nPerformance ComparationK-Fold Cross-Validated Paired t Test\nMatrix FactorizationNon-negative Matrix Factorization","categories":["Computer Science Courses"]},{"title":"Math113 Final Review","url":"/2024/12/08/Math113-Final-Pack/","content":"\n\n  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\n  \n    \n      \n      \n        Hey, password is required here.\n      \n    \n  \n\n","categories":["Math113-Abstract Algebra"]},{"title":"Projective Geometry and Transformations of 2D","url":"/2024/07/14/Projective-Geometry-and-Transformations-of-2D/","content":"Planar GeometryThe 2D Projective PlanePoints and Lines Result 2.1: The point  lies on the line  if and only if . Result 2.2: The intersection of two lines  Result 2.3: The line determined by two points is  Result 2.6: Duality principle. To any theorem of 2-dimensional projective geometry there corresponds a dual theorem, which may be derived by interchanging the roles of points and lines in the original theorem.  \nEquation of a conic:Homogenizing this by replacement ,, we haveAnd its matrix form:where C is defined as follows,\nFive points determines a conic,\nAnd we have,\nResult 2.7: The line l is tangent to C at a point x on C is given by l = Cx.  \nProjective Transformationstransformation of conicsResult 2.13. Under a point transformation , a conic  transforms to .proof:\n\nDual conic shares the same conclusion.\nA Hierarchy of TransformationDecomposition of Projective TransformationA projective transformation can be decomposed to a series of transformations, where each transformation offers one unique transform different to the former one.   \n \n \nThe projective geometry of 1DCross Ratio  \n \nRecovery of affine and metric properties from imagesThe Line at InfinityResult 2.17. The line at infinity, , is a fixed line under the projective transformation  if and only if  is an affinity. \nRecovery of affine properties from imagesOnce  is identified a length ratio on a line may be computed from the cross ratio of the three points specifying the lengths together with the intersection of the line with l∞(which provides the fourth point for the cross ratio), and so forth.Affine properties of the first plane can be measured from the third, i.e. the third plane is within an affinity of the first.    \nThe circular points and their dualInitially, if the conic is a circle, we haveThe conic intersects  at the circular points, where , we haveAnd we have Algebraically, the circular points are the orthogonal directions of Euclidean geometry,  and , packaged into a single complex conjugate entity,\neg.\nThe conic dual to the circular points isand we have    \nResult 2.22. The dual conic  is fixed under the projective transformation  if and only if  is a similarity.\nAngles on the projective plane        \nResult 2.23. Once the conic  is identified on the projective plane then Euclidean angles may be measured by the formula above.Proof.  \n\nResult 2.24. Lines  and  are orthogonal if  = 0.\nRecovery of metric properties from imagesAccording to Result 2.24, we have   which means that the projective (v) and affine (K) components are determined directly from the image of $C_{\\infty}^{} C_{\\infty}^{} C_{\\infty}^{*} $ is identified on the projective plane then projective distortion may be rectified up to a similarity.As a matter of fact, by using SVD, we have   then by inspection from (2.23) the rectifying projectivity is H = U up to a similarity.\nMetric rectification ISuppose the lines ,  in the affinely rectified image correspond to an orthogonal line pair ,  on the world plane. From result 2.24 = 0, and using (2.23) with v = 0, we have   Here,  is a symmetric matrix where . The orthogonality condition reduces  the equation which may be written as   And we can get  from the equation, futhermore  by using Cholesky Decomposition.\nMore properties of conics      A conic is an (a) ellipse, (b) parabola, or (c) hyperbola; according to whether it (a) has no real intersection, (b) is tangent to (2-point contact), or (c) has 2 real intersections with  . Under an affine transformation  is a fixed line, and intersections are preserved.Thus this classification is unaltered by an affinity.\nConclusions1.use the vanishing line to recover affine properties from images2.use metric information on the plane, such as right angles, to recover the metric geometry.\n","categories":["Multi-View Geometry"]},{"title":"VLOGGGGGG","url":"/2025/08/29/VLOGGGGGG/","content":"\n\n  0a3cef6aac56b0506dcd9a646e1e72b1029c6e9d151b8470585c39aefcd01a56b6f1f167395f5ec0d2b0b07af65162b24384c929dd777b69b732735a3484730dffd9c64e5142bb725c1ab7bc37e9f4348c68799297ea233aef7e03e8d741b3afdbddd9ce05d3337bac061116ad3c0b67c79e5793ee6abdfb251e2d571f9bc542ad095c53418e5ca964a07f962865a498b227be03aed5e466ed4fa496e3cc37ba7cfe1340493ad76767740b07897d556836596e935b9e1e996e3549a677396e213c78b173fe9615776949e4c4b9d6fcdb778d30328b5912b97ffaa8745df8c24f166f219e5ae14cd2e6079579085025a3e4ca12c1f9c59571bdcdc40a485b63e6ff48b2949554d8ac6513333c8ae62c450957937ba35995486818b7590d727c76c3c5d1881f6475ccbbac898469f6232d1e21caa248773978e026640fc0e2ad7c1cd73d22a04f7788603e59184e212280a4246cf781a1f30415cef8a1043413cf920cecf86ff0a5223c574d52742890ff57945bc0ce096e47bd5c3af6b18223a94d0f9296a1cb5caa1e8efe3414b269e7966d4b5627f79f0dcede28de168aa52bac4151715b54fbc69b91943ce53190924f2824d8f5a7cb51c2cf1bb730dea01af39042372f76a75a1cecdb524d1fa5c6413fa04f68b64d2c092ddf293d5f9bf5949847208741a24f61629773d3ad12ba13f698e4b5e5eff1f817898065b21c4a0e94a18adff9bf5b5559513ef5b66b2fd2f21540a6fa8b5a1243a51bdef76330\n  \n    \n      \n      \n        Hey, password is required here.\n      \n    \n  \n\n"},{"title":"Abstract Algebra- Basic problems for MT2","url":"/2024/11/02/midterm2-practice/","content":"Instructions:\n\nProvide clear and complete reasoning for every problem.\nUse results from lectures and notes provided, referencing definitions where appropriate.\nThe problems are divided into six parts, each focusing on specific topics covered in the notes.\nMathematical expressions are enclosed within $...$ for inline math and $$...$$ for display math.\nTotal of 30 problems are provided, each with meaningful content and appropriate difficulty.\n\n\nPart I: Group Actions and PermutationsProblem 1: Orbits and Stabilizers in Symmetric GroupsLet , the symmetric group on 4 elements, act on the set  by permutation.\n\n(a) For the element , determine the orbit  under the action of .\n\n(b) Compute the stabilizer subgroup .\n\n(c) Calculate the sizes of  and , and verify the Orbit-Stabilizer Theorem.\n\n\nHint: Use the definitions of orbit and stabilizer, and recall that .\n\nProblem 2: Group Action on SubsetsLet  act on the set  of all 2-element subsets of  by permutation.\n\n(a) How many elements are in ?\n\n(b) For the subset , determine its orbit under the action of .\n\n(c) Find the stabilizer subgroup  and compute its order.\n\n(d) Verify the Orbit-Stabilizer Theorem for this action.\n\n\nHint: Consider how permutations affect subsets and calculate accordingly.\n\nProblem 3: Orbits in Group ActionsLet  be the subgroup of  consisting of all permutations that fix the element  (i.e., ).\n\n(a) Describe all elements of .\n\n(b) Determine the orbits of  acting on .\n\n(c) Find the orbit of  and compute the size of .\n\n\nHint: Use the fact that permutations in  fix  and permute the other elements.\n\nProblem 4: Group Actions and Equivalence RelationsLet a group  act on a set . Define a relation  on  by  if there exists  such that .\n\n(a) Prove that  is an equivalence relation.\n\n(b) Show that the equivalence classes under  are precisely the orbits of  on .\n\n\nHint: Verify the properties of reflexivity, symmetry, and transitivity using the group action.\n\nProblem 5: Orbit-Stabilizer Theorem ApplicationLet  act on  by permutation: .\n\n(a) Determine the size of .\n\n(b) For the element , find .\n\n(c) Compute  and verify the Orbit-Stabilizer Theorem.\n\n\nHint: Consider the action of  on ordered pairs.\n\nPart II: Definitions and Equivalence of Group ActionsProblem 6: Group Actions via Homomorphisms\n(a) Define what it means for a group  to act on a set  via a function .\n\n(b) Show that this action defines, for each , a bijection .\n\n(c) Prove that the map  defined by  is a group homomorphism.\n\n\nHint: Use the properties of the group action and the composition of functions.\n\nProblem 7: Equivalence of Definitions\n(a) Show that any group homomorphism  defines a group action of  on  via .\n\n(b) Prove that the two definitions of group action (via  and via ) are equivalent.\n\n\nHint: Construct the correspondence between  and  explicitly.\n\nProblem 8: Kernels of Group ActionsLet  act on a set , and let  be the associated homomorphism.\n\n(a) Define the kernel of the action, .\n\n(b) Prove that .\n\n(c) Show that  is a normal subgroup of .\n\n\nHint: Use the properties of group homomorphisms.\n\nProblem 9: Faithful Group Actions\n(a) Define what it means for a group action to be faithful.\n\n(b) Prove that the action is faithful if and only if , where  is the identity in .\n\n(c) Give an example of a faithful and a non-faithful group action.\n\n\nHint: Consider the effect of elements in  on .\n\nProblem 10: Left Regular Action ExampleLet  act on itself via the left regular action.\n\n(a) Describe explicitly how the left regular action is defined for .\n\n(b) Write down the permutation representation of  corresponding to this action.\n\n(c) Show that this action is faithful.\n\n\nHint: The left regular action is given by .\n\nPart III: Left Regular Action and Cayley’s TheoremProblem 11: Cayley’s Theorem Proof\n(a) State Cayley’s Theorem.\n\n(b) Provide a detailed proof of Cayley’s Theorem using the left regular action.\n\n\nHint: Show that the mapping from  to  via the left regular action is an injective homomorphism.\n\nProblem 12: Left Regular Action of a Non-Abelian GroupLet  be the symmetric group .\n\n(a) Describe the left regular action of  on itself.\n\n(b) Write down the permutation matrices corresponding to the action.\n\n(c) Show that  is isomorphic to a subgroup of .\n\n\nHint: Since , the left regular action maps  into .\n\nProblem 13: Kernel of Left Regular ActionLet  be any group acting on itself by left multiplication.\n\n(a) Determine the kernel of this action.\n\n(b) Conclude whether the action is faithful.\n\n\nHint: Consider whether any non-identity element fixes all elements under left multiplication.\n\nProblem 14: Action of  on ItselfLet , and define an action of  on itself by .\n\n(a) Is this action faithful?\n\n(b) Find the associated homomorphism .\n\n(c) Determine the kernel of .\n\n\nHint: Analyze whether different elements of  induce different permutations.\n\nProblem 15: Cayley’s Theorem ApplicationLet , the dihedral group of order 8.\n\n(a) Describe the elements of .\n\n(b) Use the left regular action to embed  into .\n\n(c) Show that this embedding is injective.\n\n\nHint: Consider the permutations of  induced by left multiplication.\n\nPart IV: Burnside’s Lemma and Counting ColoringsProblem 16: Burnside’s Lemma Statement and Proof\n(a) State Burnside’s Lemma.\n\n(b) Provide a proof of Burnside’s Lemma using double counting.\n\n\nHint: Count the number of pairs  such that .\n\nProblem 17: Coloring Vertices of a SquareConsider coloring the vertices of a square using  colors, where two colorings are considered the same if one can be obtained from the other by a rotation or reflection (the action of ).\n\n(a) Determine the group  acting on the colorings.\n\n(b) Use Burnside’s Lemma to calculate the number of distinct colorings.\n\n(c) Compute this number explicitly for .\n\n\nHint: For each element , find the number of colorings fixed by .\n\nProblem 18: Coloring Edges of a CubeConsider coloring the edges of a cube using  colors, up to rotational symmetries.\n\n(a) Describe the rotation group  acting on the cube.\n\n(b) Determine the number of elements in .\n\n(c) Use Burnside’s Lemma to find the number of distinct colorings when .\n\n\nHint: Identify the types of rotations and calculate fixed colorings.\n\nProblem 19: Coloring with Cyclic Group ActionsConsider a necklace with  beads, and  acting by rotation.\n\n(a) Describe the action of  on the set of colorings.\n\n(b) Use Burnside’s Lemma to compute the number of distinct colorings with  colors.\n\n(c) Calculate this number explicitly for  and .\n\n\nHint: For each rotation, determine the number of colorings it fixes.\n\nProblem 20: Fixed Points under Group ActionLet  act on a finite set , and suppose that for every , the number of fixed points  is known.\n\n(a) Use Burnside’s Lemma to compute the number of orbits of  on .\n\n(b) If  and  for all , and , find the number of orbits.\n\n\nHint: Apply the formula from Burnside’s Lemma.\n\nPart V: Rings, Fields, Units, and Zero-DivisorsProblem 21: Units and Zero-Divisors in Rings\n(a) Define a unit and a zero-divisor in a ring .\n\n(b) Prove that in any ring , the set of units and the set of zero-divisors (excluding zero) are disjoint.\n\n(c) Provide an example of a ring where elements are neither units nor zero-divisors.\n\n\nHint: Consider the ring  and its elements.\n\nProblem 22: Units in Finite RingsLet .\n\n(a) List all units in .\n\n(b) List all zero-divisors in .\n\n(c) Verify that  consists of units, zero-divisors, and zero.\n\n\nHint: An element  is a unit if .\n\nProblem 23: Units in Gaussian IntegersConsider the ring  of Gaussian integers.\n\n(a) Define what it means for an element to be a unit in .\n\n(b) Find all units in .\n\n(c) Prove that  is not a unit in .\n\n\nHint: Use the norm .\n\nProblem 24: Zero-Divisors in Rings\n(a) Give an example of an infinite ring with zero-divisors.\n\n(b) Show that in the ring , there exist zero-divisors.\n\n(c) Provide an explicit example of two non-zero matrices  and  such that .\n\n\nHint: Consider matrices where the product results in the zero matrix.\n\nProblem 25: Division Algorithm in \n(a) State the Division Algorithm in .\n\n(b) Given integers  and , find integers  and  such that  and .\n\n(c) Verify that the values of  and  satisfy the conditions.\n\n\nHint: Perform integer division and find the remainder.\n\nProblem 26: Division Algorithm in \n(a) State the Division Algorithm for polynomials in .\n\n(b) Divide  by  and find the quotient and remainder.\n\n(c) Verify that the degree of the remainder is less than the degree of .\n\n\nHint: Be careful with integer coefficients during polynomial division.\n\nProblem 27: Division Algorithm in Let  and  in .\n\n(a) Use the Division Algorithm in  to find  and  such that  with .\n\n(b) Compute the norm of  and verify the inequality.\n\n(c) Show all steps of your calculation.\n\n\nHint: Find the closest Gaussian integer to .\n\nProblem 28: Units and Zero-Divisors in Let  be a positive integer.\n\n(a) Describe the units in the ring .\n\n(b) Explain why  is a zero-divisor in  if  and .\n\n(c) For , list all units and zero-divisors in .\n\n\nHint: Use properties of modular arithmetic.\n\nProblem 29: Units in Polynomial Rings over FieldsLet , the ring of polynomials with rational coefficients.\n\n(a) Determine all units in .\n\n(b) Is the polynomial  a unit in ? Justify your answer.\n\n(c) Explain why polynomials of degree zero are units if they are non-zero.\n\n\nHint: Units in  are the invertible elements under multiplication.\n\nProblem 30: Non-Commutative Rings and UnitsConsider the ring .\n\n(a) Determine the units in .\n\n(b) Explain why  is non-commutative.\n\n(c) Is every non-zero element of  either a unit or a zero-divisor? Justify your answer.\n\n\nHint: Recall that invertible matrices have non-zero determinants.\nMidterm 2 Practice Problems: Division Algorithm/GCD of Polynomials and Burnside’s Lemma\nInstructions:\n\nProvide clear and complete reasoning for every problem.\nUse results from lectures and the provided notes, referencing definitions where appropriate.\nThe problems are designed to be challenging and integrate multiple concepts from your course.\nMathematical expressions are enclosed within $...$ for inline math and $$...$$ for display math.\nEach section contains 10 problems focused on the specified topic.\nProblems are intended to be comprehensive and computationally intensive, suitable for exam preparation.\n\n\nPart I: Division Algorithm and GCD of PolynomialsProblem 1: Division Algorithm in Let  and  in .\n\n(a) Use the Division Algorithm to divide  by , and find the quotient  and remainder .\n\n(b) Verify that .\n\n(c) Determine the degrees of  and .\n\n\nHint: Be careful with polynomial long division involving rational coefficients.\n\nProblem 2: Euclidean Algorithm in Let  and  in .\n\n(a) Use the Euclidean Algorithm to compute the greatest common divisor .\n\n(b) Express  as a linear combination of  and .\n\n(c) Determine whether  and  are coprime.\n\n\nHint: Perform successive divisions and track the remainders.\n\nProblem 3: Factorization in Let  in .\n\n(a) Factor  completely over .\n\n(b) Use the Division Algorithm to confirm one of the factors.\n\n(c) Find the .\n\n\nHint: Consider factoring  into quadratic or linear factors.\n\nProblem 4: Irreducibility and GCDLet  and  in .\n\n(a) Show that  is irreducible over .\n\n(b) Compute the .\n\n(c) Explain why  and  are coprime.\n\n\nHint: Use Eisenstein’s Criterion for irreducibility.\n\nProblem 5: Division Algorithm in  with Modulo ArithmeticLet  and  in .\n\n(a) Perform the division of  by  in .\n\n(b) Find the quotient and remainder.\n\n(c) Verify the Division Algorithm in this ring.\n\n\nHint: Remember that coefficients are modulo .\n\nProblem 6: GCD of Polynomials over Finite FieldsLet  and  in , where  is the finite field with two elements.\n\n(a) Compute  in .\n\n(b) Factor  and  completely over .\n\n(c) Determine whether  and  are coprime.\n\n\nHint: In , addition and subtraction are the same.\n\nProblem 7: Application of Remainder TheoremLet  in .\n\n(a) Use the Remainder Theorem to find the remainder when  is divided by .\n\n(b) Confirm your result using the Division Algorithm.\n\n(c) Determine if  is a factor of .\n\n\nHint: Evaluate  for the remainder.\n\nProblem 8: Extended Euclidean AlgorithmLet  and  in .\n\n(a) Use the Extended Euclidean Algorithm to find polynomials  and  such that .\n\n(b) Interpret the result in terms of .\n\n(c) Determine if  and  are coprime.\n\n\nHint: The Extended Euclidean Algorithm generalizes to polynomials.\n\nProblem 9: Content of a PolynomialDefine the content of a polynomial  as the greatest common divisor of its coefficients.\nLet .\n\n(a) Compute the content of .\n\n(b) Find the primitive part of  (divide by its content).\n\n(c) Factor the primitive part over .\n\n\nHint: Factor out the  of the coefficients first.\n\nProblem 10: Gauss’s Lemma and GCDLet  and  in .\n\n(a) Show that the product  is primitive.\n\n(b) Compute .\n\n(c) Use Gauss’s Lemma to explain your findings.\n\n\nHint: Recall that the product of primitive polynomials is primitive.\n\nPart II: Burnside’s Lemma Applications (Without Polyhedra)Problem 11: Coloring a Necklace with BeadsConsider a necklace with  beads, and  colors available to color each bead. Two necklaces are considered the same if one can be obtained from the other by rotation (no reflections).\n\n(a) Describe the cyclic group  acting on the set of necklaces.\n\n(b) Use Burnside’s Lemma to compute the number of distinct necklaces.\n\n(c) Calculate this number explicitly for  and .\n\n(d) Determine the number of necklaces fixed by a rotation of .\n\n\nHint: For each rotation, compute the number of colorings it fixes.\n\nProblem 12: Coloring Squares on a GridConsider a  grid of squares, and you have  colors to color each square. Two colorings are considered the same if they can be obtained from one another by rotation of  (only considering rotation by ).\n\n(a) Describe the group  acting on the set of colorings.\n\n(b) Use Burnside’s Lemma to find the number of distinct colorings.\n\n(c) Calculate this number explicitly for .\n\n(d) Determine the number of colorings fixed by the  rotation.\n\n\nHint: The group  has two elements: the identity and the  rotation.\n\nProblem 13: Permutations and ColoringsLet  act on the set of colorings of four objects using  colors, where two colorings are considered the same if they can be obtained from one another by a permutation in .\n\n(a) Describe the action of  on the colorings.\n\n(b) Use Burnside’s Lemma to compute the number of distinct colorings.\n\n(c) Compute this number explicitly for .\n\n(d) Determine the number of colorings fixed by a permutation of cycle type .\n\n\nHint: Identify the cycle types and compute fixed colorings accordingly.\n\nProblem 14: Coloring Vertices of a PolygonConsider a regular -gon in the plane, and you want to color its vertices using  colors. Two colorings are considered the same if they can be obtained from one another by rotation (no reflections).\n\n(a) Describe the cyclic group  acting on the vertices.\n\n(b) Use Burnside’s Lemma to compute the number of distinct vertex colorings.\n\n(c) Calculate this number explicitly for  and .\n\n(d) Determine the number of colorings fixed by a rotation of .\n\n\nHint: The rotation group has  elements corresponding to rotations.\n\nProblem 15: Coloring Strings with RepetitionConsider strings of length  formed from an alphabet of  letters. Two strings are considered the same if one can be transformed into the other by reversing the string (palindrome consideration).\n\n(a) Describe the group  acting on the set of strings.\n\n(b) Use Burnside’s Lemma to find the number of distinct strings under this equivalence.\n\n(c) Compute this number explicitly for  and .\n\n(d) Determine the number of strings fixed under the reversal.\n\n\nHint: The group has two elements: the identity and the reversal.\n\nProblem 16: Coloring Cells in a Circular ArrayConsider a circular array of  cells, each of which can be colored with  colors. Two colorings are considered the same if they can be rotated or reflected (dihedral group ).\n\n(a) Describe the dihedral group  acting on the colorings.\n\n(b) Use Burnside’s Lemma to compute the number of distinct colorings.\n\n(c) Calculate this number explicitly for  and .\n\n(d) Determine the number of colorings fixed by a reflection.\n\n\nHint: Account for both rotations and reflections in your calculations.\n\nProblem 17: Coloring Positions in a Dance CircleIn a dance circle with  positions, each dancer wears a dress of one of  colors. Two arrangements are considered the same if they can be obtained from one another by rotation.\n\n(a) Describe the cyclic group  acting on the arrangements.\n\n(b) Use Burnside’s Lemma to compute the number of distinct arrangements.\n\n(c) Calculate this number explicitly for  and .\n\n(d) Determine the number of arrangements fixed by a rotation of .\n\n\nHint: Similar to necklace counting but applied to dancers.\n\nProblem 18: Coloring Squares in a Magic SquareConsider arranging numbers from  to  in an  grid such that two arrangements are considered the same if they can be obtained from one another by rotation or reflection.\n\n(a) Describe the group  acting on the set of arrangements.\n\n(b) Use Burnside’s Lemma to estimate the number of distinct arrangements (this may be complex; focus on understanding the method).\n\n(c) Discuss the difficulties in computing this number explicitly.\n\n(d) For a simplified case with , compute the number of distinct arrangements.\n\n\nHint: This problem is more theoretical; consider permutations and symmetries.\n\nProblem 19: Coloring Cells in a HoneycombConsider a finite honeycomb lattice in the plane with  hexagonal cells, each of which can be colored using  colors. Two colorings are considered the same if they can be obtained from one another by translations and rotations within the plane.\n\n(a) Describe the symmetry group  acting on the colorings.\n\n(b) Use Burnside’s Lemma to compute the number of distinct colorings.\n\n(c) Explain why this calculation may be complex.\n\n(d) For a simplified case with  cells forming a triangle, compute the number of distinct colorings for .\n\n\nHint: Focus on the symmetries of the small configuration.\n\nProblem 20: Coloring Lattice Points in the PlaneConsider a finite grid of  points in the plane, and each point can be colored using  colors. Two colorings are considered the same if they can be obtained from one another by a rotation of  about the center of the grid.\n\n(a) Describe the group  acting on the colorings.\n\n(b) Use Burnside’s Lemma to compute the number of distinct colorings.\n\n(c) Compute this number explicitly for  and .\n\n(d) Determine the number of colorings fixed by the  rotation.\n\n\nHint: Only consider the identity and the  rotation.\n\nEnd of Practice Problems\n","categories":["Math113-Abstract Algebra"]}]